\part[author={\protect\insertauthor},label={chap:syntax_analysis}]{Syntax Analysis}

\begin{graphicspathcontext}{{./chapters/chapter3/imgs/auto/},{./chapters/chapter3/imgs/raw/},{./chapters/chapter1/imgs/auto/}}
\begin{bibunit}[apalike]
	
\tableofcontentslide

\section{Introduction}
\sectiontableofcontentslide

\subsection{General principles}

\sidecite{Chomsky.1956, Backus.1959, Naur.1963}
\begin{rightlawnframe}<11>{{Syntax Analyzer} or Parser}{syntax-analyzer-introduction}
	\putat*(340,-197){\includeanimatedfigure[width=2.5cm]{compiler_structure}}
	Every programming language has precise rules that prescribe the syntactic structure of well-formed programs \\[.25cm]
	Language syntax is specified by \emph{context-free grammars} or \emph{Backus-Naur Form} (BNF) \\[.25cm]
	Reads a stream of tokens \\[.25cm]
	Do a semantic analysis \\[.25cm]
	Outputs a syntax tree
\end{rightlawnframe}

\begin{frame}{{What is a Language Grammar?} What is Language Semantics?}
	\begin{definitionblock}{Syntax}
		Set of rules that defines the combinations of symbols that are considered to be correctly structured statements or expressions in that language
	\end{definitionblock}
	\vspace{.25cm}
	\begin{definitionblock}{Grammar}
		For text-based computer languages, a grammar gives a precise, easy-to-understand, syntactic specification of a programming language
	\end{definitionblock}
	\vspace{.25cm}
	\begin{definitionblock}{Semantics}
		Syntax therefore refers to the form of the code, and is contrasted with semantics: \emph{the meaning}
	\end{definitionblock}
\end{frame}

\sidecite{Younger.1967, Kasami.1965, Earley.1970}
\begin{frame}{Types of Syntax Analyzers}
	\centering
	\fancybox{Universal Parsers}{Build syntax tree as a whole}{universal-icon}{}
	\hspace{.5cm}
	\fancybox{Top-down Parsers}{Build the syntax tree from the root rule to tokens}{top-down-icon}{}
	\hspace{.5cm}
	\fancybox{Bottom-up Parsers}{Build the syntax tree from the tokens to the root rule}{bottom-up-icon}{}
\end{frame}

\sidecite{Younger.1967, Kasami.1965, Earley.1970}
\begin{frame}{Types of Syntax Analyzers \insertcontinuationtext}
	\centering
	\fancybox[bg=red,fg=black,titlefg=CIADlightmagenta]{Universal Parsers}{Algorithms, e.g., Cocke-Younger-Kasami and Earley, are too inefficient}{universal-icon}{}
	\hspace{.5cm}
	\fancybox{Top-down Parsers}{\textcolor{red}{Efficient with Left-Left ($LL$) and Left-Right ($LR$) grammars}}{top-down-icon}{}
	\hspace{.5cm}
	\fancybox{Bottom-up Parsers}{\textcolor{red}{Efficient with Left-Left ($LL$) and Left-Right ($LR$) grammars}}{bottom-up-icon}{}
	\putat*(20, -185){
		\normalsize\normalfont\usebeamerfont{normal text}\usebeamercolor[fg]{normal text}
		$LL \rightarrow$ writing a parser by hands |
		$LR \rightarrow$ automatic generation of parser
	}
\end{frame}

\subsection{Error recovery}

\sidecite{Dain.1991}
\begin{frame}{{Syntax Error} Handling}
	\begin{rightarrowsequence}
		\arrow[bg=CIADgreen]{Compiler assists programmers in locating and tracking errors}
		\arrow[bg=CIADmagenta]{
			Most programming language specifications do not describe how a compiler should respond to errors \\[.25cm]
			Error handling is left to the compiler designer
		}
	\end{rightarrowsequence}
\end{frame}

\sidecite{Dain.1991}
\begin{frame}[background=8]{{Why} handling errors during the parsing?}
	\larger
	\begin{enumerate}
		\item $LL$ and $LR$ methods permits to detect errors efficiently and as soon as possible
		\vspace{1cm}
		\item Many errors appear syntactic, whatever they cause, and avoid the code generation
	\end{enumerate}
\end{frame}

\sidecite{Dain.1991}
\begin{frame}{{Missions} of the Error Handler}
	\fancybox{Report errors}{Report the presence of errors clearly and accurately}{debug-icon}{01}
	\hfill
	\fancybox{Recover consistent state}{Recover from each error quickly to detect subsequent errors}{recover-icon}{02}
	\hfill
	\fancybox{Efficient}{Add minimal overhead to the processing of correct programs}{software-efficiency}{03}
\end{frame}

\begin{frame}{Types of Programming Errors}
	\begin{columns}
		\begin{column}[t]{.5\linewidth}
			\begin{block}{Lexical Errors}
				\emph{Invalid lexemes}, e.g., misspellings of identifiers, keywords, or operators; and missing quotes around text intended as a string
			\end{block}
		\end{column}
		\begin{column}[t]{.5\linewidth}
			\begin{block}{Syntactic errors}
				\emph{Invalid tokens} that are broking the grammar, e.g., misplaced semicolons or extra or missing braces. Another example is a \kw{case} outside an enclosing \kw{switch} block
			\end{block}
		\end{column}
	\end{columns}
	\vspace{.25cm}
	\begin{columns}
		\begin{column}[t]{.5\linewidth}
			\begin{block}{Semantic errors}
				\emph{Incorrect usage} of the language elements in the syntax tree, e.g., type mismatches between operators and operands
			\end{block}
		\end{column}
		\begin{column}[t]{.5\linewidth}
			\begin{block}{Logical errors}
				\emph{Incorrect reasoning} of the programmer, e.g., the use of the operator \code{"="} in place of the operator \code{"=="}; or unreachable code
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Strategies for Error Recovery}
	\alertbox{Once an error is detected, how should the parser recover?}
	\vspace{.25cm}
	\begin{rightarrowsequence}
		\arrow{Simplest approach: parser quits with an informative error message when it detects the first error; additional errors are uncovered}
		\arrow{If errors are piled up, compiler stops after exceeding some limit}
	\end{rightarrowsequence}	
	\vspace{.5cm}
	\alertbox*{Two error recovery strategies are usually used:}
	\begin{center}
	\viconbox*[.25\linewidth]{Panic Mode}{panic-mode-icon}
	\hspace{2cm}
	\viconbox*[.25\linewidth]{Phrase-level Mode}{phrase-level-mode-icon}
	\end{center}
\end{frame}

\begin{leftlawnframe}{Panic Mode}{panic-mode}
	Parser discards input symbols one at a time until one of a designated set of synchronizing tokens is found \\[.5cm]
	Synchronizing tokens are usually delimiters (semicolons or closing braces) \\[.5cm]
	Simple to implement and ensure not to go into an infinite loop
\end{leftlawnframe}

\begin{frame}{Phrase-Level Recovery}
	\begin{block}{Local correction on the remaining input}
		Replace the prefix of the remaining input by some string that allows the parser to continue
		%; eg. 
	\end{block}
	\begin{examples}
		Replacing a coma by a semicolon, remove extraneous semicolon, or insert a missed semicolon
	\end{examples}
	\vspace{1cm}
	\hiconbox{
		The major drawback of the phrase-level recovery is the difficulty it has in coping with situations in which the actual error has occurred before the point of detection
	}{problem-icon}
\end{frame}

\begin{frame}{Why are Global Corrections Discarded?}
	\begin{block}{Principle of Global Corrections}
		\begin{itemize}
			\item Given an incorrect input string \ccode{x} and grammar $G$
			\item Find a syntax tree for a related string \ccode{y}
			\item Condition: number of insertions, deletions, and changes of tokens required to transform \ccode{x} to \ccode{y} is as small as possible
		\end{itemize}
	\end{block}
	\vspace{.25cm}
	\alertbox{This method is usually too costly in time and space}
	\vspace{.25cm}
	\begin{block}{Global corrections has been used to}
		\begin{itemize}
			\item Evaluate error-recovery algorithms
			\item Find optimal replacement strings for phrase-level recovery
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{{Error Productions} for Anticipated Error Detection}
	\begin{rightarrowsequence}
		\arrow{Augment the grammar with rules/productions that generate the erroneous constructs}
		\arrow{Detect error when error production is used during parsing}
		\arrow{Generate appropriate error diagnostics with appropriate lexemes}
	\end{rightarrowsequence}
\end{frame}

\section{Context-free grammar}

\subsection{Definition and notation}
\subsectiontableofcontentslide
	
\sidecite{Chomsky.1956, Backus.1959, Naur.1963, Ingerman.1967, Hopcroft.2006}
\begin{frame}[t]{What is a Formal Grammar?}
	\alertbox*{A formal grammar consists of \emph{productions}, that consist of \emph{terminals}, \emph{nonterminals}; and a \emph{start symbol}}
	\vspace{.25cm}
	\begin{center}
		\Large
		\begin{tabular}{rcl}
			\textcolor<4-5>{CIADgreen}{\bnftd{S}} & \textcolor<4>{CIADgreen}{\bnfopo} & \textcolor<4>{CIADgreen}{\textcolor<2>{CIADgreen}{\bnfts{a}}\hspace{1em}\textcolor<3>{CIADgreen}{\bnftd{S}}\hspace{1em}\textcolor<2>{CIADgreen}{\bnfts{b}}} \\
			\bnftd{S} & \bnfopo & \textcolor<2>{CIADgreen}{\bnfts{b}}\hspace{1em}\textcolor<2>{CIADgreen}{\bnfts{a}} \\
		\end{tabular}
	\end{center}
	\only<2>{\begin{definitionblock}{Terminal --- Token Name}
		The basic symbols from which strings are formed \\
		It could be assimilated to a token, replied by the lexical analyzer (see Chapter~\ref{chap:lexical_analysis})
	\end{definitionblock}}
	\only<3>{\begin{definitionblock}{Nonterminals}
		Syntactic variables that denote sets of strings that generate the language \\
		Nonterminals impose a hierarchical structure on the language \\
		Nonterminals must be defined in the grammar itself
	\end{definitionblock}}
	\only<4>{\begin{definitionblock}{Production}
		Productions specify the manner in which the terminals and nonterminals can be combined to form strings \\
		Each production consists of: \begin{enumerate}
			\item A nonterminal called the \emph{head} or left side
			\item The symbol ``\bnfopo'' (or ``\bnfpo'', or ``$\models$'')
			\item A \emph{body}, or right side, consisting of zero or more terminals and nonterminals, describing a replacement for the head
		\end{enumerate}
	\end{definitionblock}}
	\only<5>{\begin{definitionblock}{Start Symbol}
			Nonterminal from which all the language's strings could be revided \\
			Conventionally: the head of the first production is the start symbol
		\end{definitionblock}}
\end{frame}

\sidecite{Chomsky.1956}
\begin{frame}[t,background=9]{{Formal Definition} of a Grammar by Chomsky}
	\[
		G = \langle N, \Sigma, P, S\rangle
	\]
	\begin{itemize}
		\item Finite set $N$ of nonterminal symbols, that is disjoint with the strings formed from $G$
		\item Finite set $\Sigma$ of terminal symbols that is disjoint from $N$
		\item Finite set $P$ of production rules, each rule of the form $(\Sigma \cup N)^* N(\Sigma \cup N)^* \bnfopo (\Sigma \cup N)^*$ \\
		where $*$ is the Kleene star operator
		\item Symbol $S \in N$ that is the start symbol, also called the sentence symbol
		\item Empty string is represented by $\bnfes$ (or $\lambda$, or $\perp$)
	\end{itemize}
	\vspace{1cm}
	\alertbox*{The language of $G$, denoted as $L(G)$, is defined as the set of sentences built by $G$}
\end{frame}

\begin{frame}[t]{Notational Conventions for Grammars}
	\begin{block}{Terminals}
		\begin{itemize}
		\item Lowercase letters early in the alphabet, such as a, b, c, \dots
		\item Operator symbols, such as +, *, \dots
		\item Punctuation symbols, such as parentheses, commas, \dots
		\item Digits 0, \dots, 9
		\item Boldface strings, such as \bnfts{id} or \bnfts{number}
		\item Underlined strings, such as \underline{id} or \underline{number}
		\end{itemize}
	\end{block}
	\begin{block}{Nonterminals}
		\begin{itemize}
		\item Uppercase letters, such as A, B, C, \dots
		\item The letter \textit{S} which, when it appears, is usually the start symbol
		\item Lowercase, italic names, such as \textit{expression}, \textit{factor}, \dots
		\item Enclosed names, e.g. $\bnfpn{expression}$
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{Notational Conventions for Grammars \insertcontinuationtext}
	\begin{block}{Productions}
		A set of productions $A \bnfopo a_1, A \bnfopo a_2, \dots, A \bnfopo a_k$ with a common head $A$ (call them $A$-productions), may be written $A \bnfopo a_1 \bnfor a_2 \bnfor \dots \bnfor a_k$
	\end{block}
	\vspace{1cm}
	\begin{block}{Others Notations}
		\begin{itemize}
			\item Uppercase letters late in the alphabet, such as X, Y, Z, represent grammar symbols that is, either nonterminals or terminals
			\item Lowercase letters late in the alphabet, chiefly u, v, \dots, z, represent (possibly empty) strings of terminals
			\item Lowercase Greek letters $\alpha$, $\beta$, \dots, represent (possibly empty) strings of grammar symbols
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{{General Principle} of Parsing}
	\begin{definitionblock}{Parsing}
		Parsing is the process of taking a string of terminals and figuring out how to \Emph{derive} it from the start symbol \\
		If the string cannot be derived, the parser reports a syntax error
	\end{definitionblock}
	\vspace{.5cm}
	\begin{rightarrowsequence}
		\arrow{Grammar derives strings by beginning with the start symbol}
		\arrow{Repeated replacement of a nonterminal by the body of a production for that nonterminal}
		\arrow{Terminal strings, that can be derived, form the language defined by the grammar, namely, $L(G)$}
	\end{rightarrowsequence}
\end{frame}

\begin{frame}[t,fragile,background={10}]{Example of a Grammar}
	\begin{itemize}
	\item Arithmetic expressions are defined by the following grammar.
	\item The terminals are: \begin{description}
		\item[Operators] \bnfts{+}, \bnfts{-}, \bnfts{*}, \bnfts{/}, \bnfts{(}, \bnfts{)}
		\item[Numbers] \bnfts{number} stands for any number
		\item[Identifier] \bnfts{id} stands for any variable's name
		\end{description}
	\item The grammar is:
	\end{itemize}
	\begin{bnf}
		\bnfprod*{expression}{\bnfpn{expression} \bnfts{+} \bnfpn{term}} \\
		\bnfalt*{\bnfpn{expression} \bnfts{-} \bnfpn{term}} \\
		\bnfprod*{term}{\bnfpn{term} \bnfts{*} \bnfpn{factor}} \\
		\bnfalt*{\bnfpn{term} \bnfts{/} \bnfpn{factor}} \\
		\bnfalt*{\bnfpn{factor}} \\
		\bnfprod*{factor}{\bnfts{(} \bnfpn{expression} \bnfts{)}} \\
		\bnfalt*{\bnfts{number}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}
\end{frame}

\sidecite{Chomsky.1956}
\begin{rightlawnframe}<11>{{Hierarchy of Grammars} by Chomsky: Type-0}{chomsky-hierarchy}
	Type-0 grammars generates languages that can be recognized by a Turing machine \\[.5cm]
	$\gamma \bnfopo \alpha$ \\[.5cm]
	\inlineexample{$L=\left\{w|w\right\}$ describes a terminating Turing machine} \\[.5cm]
	Recognition Complexity: NP-hard
\end{rightlawnframe}

\sidecite{Chomsky.1956}
\begin{rightlawnframe}<11>{{Hierarchy of Grammars} by Chomsky: Type-1}{chomsky-hierarchy}
	Type-1 grammars generate context-sensitive languages \\[.5cm]
	$\alpha A\beta \rightarrow \alpha \gamma \beta$ \\
	with $\gamma\neq\epsilon$ \\[.5cm]
	\inlineexample{$L=\left\{a^nb^nc^n|n>0\right\}$} \\[.5cm]
	\inlineexample{natural languages} \\[.5cm]
	Recognition Complexity: NP-hard
\end{rightlawnframe}

\sidecite{Chomsky.1956}
\begin{rightlawnframe}<11>{{Hierarchy of Grammars} by Chomsky: Type-2}{chomsky-hierarchy}
	Type-2 grammars generate the context-free languages \\[.5cm]
	$A \rightarrow \alpha$ \\[.5cm]
	\inlineexample{$L=\left\{a^nb^n|n>0\right\}$} \\[.5cm]
	\inlineexample{Most of programming languages} \\[.5cm]
	Recognition Complexity: $O(n^3)$
	\putat(15,0){\parbox{2.5cm}{\alert{The rest of this lecture focuses on context-free grammars}}}
\end{rightlawnframe}

\sidecite{Chomsky.1956}
\begin{rightlawnframe}<11>{{Hierarchy of Grammars} by Chomsky: Type-3}{chomsky-hierarchy}
	Type-3 grammars generate the regular languages \\[.5cm]
	$A \rightarrow \bnfts{a}$ \\
	$A \rightarrow \bnfts{a}B$\\[.5cm]
	\inlineexample{$L=\left\{a^n|n\ge0\right\}$} \\[.5cm]
	\inlineexample{Regex pattern specification} \\[.5cm]
	Recognition Complexity: $O(n)$
\end{rightlawnframe}

\subsection{Derivations and Parse Tree}
\subsubsectiontableofcontentslide*

\subsubsection{Derivations for a Grammar}

\begin{frame}{{Production Rule} Application}
	\begin{definitionblock}{Production Rule Application}
		Rule application replaces the production's nonterminal by its body \\[.2cm]
			\Emph{Formally:} string $v$ is a result of applying the rule $\alpha\bnfopo\beta$ to string $u$ if $\exists \alpha\bnfopo\beta\in P$ and $\exists u_{1},u_{2}\in (N\cup \Sigma )^{*}$, such that $u=u_{1}\alpha u_{2}$ and $v=u_{1}\beta u_{2}$ \\[.2cm]
			\Emph{Notation: } $\alpha \deriv \beta$
	\end{definitionblock}
	\begin{example}
		\begin{columns}
			\begin{column}{.3\linewidth}
				\begin{bnf}
					\bnfprod*{E}{\bnfpn{E} \bnfts{+} \bnfpn{E}} \\
					\bnfalt*{\bnfpn{E} \bnfts{*} \bnfpn{E}} \\
					\bnfalt*{\bnfts{-} \bnfpn{E}} \\
					\bnfalt*{\bnfts{(} \bnfpn{E} \bnfts{)}} \\
					\bnfalt*{\bnfts{id}}
				\end{bnf}
			\end{column}
			\begin{column}{.7\linewidth}
				\begin{itemize}
					\item Input is: \ccode{-variable}
					\item The replacement of $\bnfpn{E}$ by $\bnfts{-} \bnfpn{E}$ will be described by writing:
					\begin{center}
						$\bnfpn{E} \deriv \bnfts{-} \bnfpn{E}$
					\end{center}
					\item It means: \emph{$\bnfpn{E}$ derives in one step to $\bnfts{-} \bnfpn{E}$}
				\end{itemize}
			\end{column}
		\end{columns}
	\end{example}
\end{frame}

\begin{frame}{Derivation or Repetitive Rule Application}
	\begin{definitionblock}{Repetitive Rule Application}
		Sequence of derivations $u_1 \deriv u_2 \deriv \dots \deriv u_k$ rewrites $u_1$ to $u_n$ \\[.2cm]
		\Emph{Formally:} string $u$ derives to string $v$ if $\exists k\in\mathbb{N}+$ and $\exists u_{1},\cdots ,u_{k}\in (N\cup \Sigma)^{*}$ such that $u_{1}\deriv u_{2}\deriv \cdots \deriv u_{k}$, $u=u_1$ and $v=u_k$ \\[.2cm]
		\Emph{Notation 1:} $u_1 \seqderiv u_k$ (reflexive transitive closure)\\
		\Emph{Notation 2:} if $k\ge2$, $u_1 \seqderivone u_k$ (transitive closure)
	\end{definitionblock}
	\begin{block}{Properties}
		\begin{description}
		\item[Identity] $\alpha\seqderiv\alpha$, for any string $\alpha$
		\item[Transitivity] If $\alpha\seqderiv\beta$, and $\beta\deriv\gamma$, then $\alpha\seqderiv\gamma$
		\end{description}
	\end{block}
\end{frame}

\begin{frame}[background=8]{Derivation as a Sentence of a Grammar}
	\begin{itemize}
	\item If $S \seqderiv a$, where $\bnfpn{S}$ is the start symbol of a grammar $G$, we say that $\bnftd{a}$ is a \emph{sentential form of $G$}
	\vfill
	\item A sentence of $G$ is a sentential form, which is nonterminal
	\vfill
	\item The language generated by $G$ is its set of sentences
	\vfill
	\item A string of terminals $\bnfts{w}$ is in $L(G)$ iff $S \seqderiv w$ \\
		Thus $L(G)$ is said to be a context-free language
	\end{itemize}
\end{frame}

\begin{frame}{{Leftmost and Rightmost} Derivations}
	At each step in a derivation, there are two choices to be made:
		\begin{enumerate}
		\item Choose which nonterminal to replace
		\item Pick a production with that nonterminal as head
		\end{enumerate}
	\begin{center}
		\fancybox{Leftmost Derivation}{Leftmost nonterminal is always chosen \\[.2cm]
			{\Large$\alpha$\derivlm$\beta$}}{left-most-icon}{}
		\hspace{2cm}
		\fancybox{Rightmost Derivation}{Rightmost nonterminal is always chosen \\[.2cm]
			{\Large$\alpha$\derivrm$\beta$}}{right-most-icon}{}
	\end{center}
\end{frame}

\begin{frame}[t]{Example of Derivations}
	\begin{columns}
		\begin{column}[t]{.3\linewidth}
			\vspace{-.55cm}
			\begin{itemize}
			\item \Emph{Grammar:}
				\begin{small}\begin{bnf}
				\bnfprod*{E}{\bnfpn{E} \bnfts{+} \bnfpn{E}} \\
				\bnfalt*{\bnfpn{E} \bnfts{*} \bnfpn{E}} \\
				\bnfalt*{\bnfts{(} \bnfpn{E} \bnfts{)}} \\
				\bnfalt*{\bnfts{id}}
			\end{bnf}\end{small}
			\item Input string: \\
				\ccode{2 + 4 * 6}
			\item List of tokens: \\
				$\bnfts{id}\bnfts{+}\bnfts{id}\bnfts{*}\bnfts{id}$
			\end{itemize}
		\end{column}
		\begin{column}[t]{.3\linewidth}
			\Emph{Left-most Derivations:}
			\begin{tabular}[t]{@{}p{1em}p{1em}l}
				$\bnfpn{E}$ & \derivlm & $\bnfpn{E} \bnfts{+} \bnfpn{E}$ \\
				\only<2->{& \derivlm & $\bnfts{id} \bnfts{+} \bnfpn{E}$ \\}
				\only<3->{& \derivlm & $\bnfts{id} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$ \\}
				\only<4->{& \derivlm & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfpn{E}$ \\}
				\only<5->{& \derivlm & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfts{id}$ \\}
			\end{tabular}
		\end{column}
		\begin{column}[t]{.3\linewidth}
			\Emph{Right-most Derivations:}
			\begin{tabular}[t]{@{}p{1em}p{1em}l}
				$\bnfpn{E}$ & \derivrm & $\bnfpn{E} \bnfts{+} \bnfpn{E}$ \\
				\only<2->{& \derivrm & $\bnfpn{E} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$ \\}
				\only<3->{& \derivrm & $\bnfpn{E} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfts{id}$ \\}
				\only<4->{& \derivrm & $\bnfpn{E} \bnfts{+} \bnfts{id} \bnfts{*} \bnfts{id}$ \\}
				\only<5->{& \derivrm & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfts{id}$ \\}
			\end{tabular}
		\end{column}
	\end{columns}
	\only<6>{%
		\vspace{.5cm}
		\alertbox*{Grammar is ambiguous because the following derivation is possible on the input: \\
			$\bnfpn{E} \derivlm \bnfpn{E} \bnfts{*} \bnfpn{E}$
			$\derivlm \bnfpn{E} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$
			$\derivlm \bnfts{id} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$
			$\derivlm \bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfpn{E}$
			$\derivlm \bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfts{id}$
		}
	}
\end{frame}

\subsubsection{Parse Tree}
\subsubsectiontableofcontentslide

\begin{frame}{{What is a} Parse Tree?}
	\begin{definitionblock}{Parse Tree}
	A parse tree shows how the start symbol of a grammar derives a string \\
	It is a graphical representation of the productions on an input string of tokens
	\end{definitionblock}
	\begin{columns}
		\begin{column}{.4\linewidth}
			\includegraphicswtex[width=\linewidth]{example_FIRST}
		\end{column}
		\begin{column}{.6\linewidth}
			\begin{itemize}
				\item The root is labeled by the start symbol
				\item Each leaf is labeled by a terminal or by $\bnfes$
				\item Each interior node is labeled by a nonterminal
				\item If $A$ is the nonterminal of some interior node and $X_1, X_2, \dots, X_n$ are the labels of the children of that node from left to right, then there must be a production $A \bnfopo X_1 X_2 \dots X_n$
			\end{itemize}
		\end{column}
	\end{columns}
	\vspace{.5cm}
	\alertbox{Parsing is the process of building a parse tree from a string of tokens}
\end{frame}

\begin{frame}{Example of a Parse Tree}
	\begin{columns}
		\begin{column}[t]{.5\linewidth}
			Let the string to parse:
				\begin{center}\texttt{9 - 5 + 2}\end{center}
			Let the grammar:\\[1em]
			\begin{scriptsize}
			\begin{bnf}
				\bnfprod*{expression}{\bnfpn{expression} \bnfts{+} \bnfpn{term}} \\
				\bnfalt*{\bnfpn{expression} \bnfts{-} \bnfpn{term}} \\
				\bnfprod*{term}{\bnfpn{term} \bnfts{*} \bnfpn{factor}} \\
				\bnfalt*{\bnfpn{term} \bnfts{/} \bnfpn{factor}} \\
				\bnfalt*{\bnfpn{factor}} \\
				\bnfprod*{factor}{\bnfts{(} \bnfpn{expression} \bnfts{)}} \\
				\bnfalt*{\bnfts{number}} \\
				\bnfalt*{\bnfts{id}}
			\end{bnf}
			\end{scriptsize}
		\end{column}
		\begin{column}[t]{.5\linewidth}
			The parse tree is:\\[1em]
			\includegraphics[width=.9\linewidth]{parse_tree_expr}
		\end{column}			
	\end{columns}
\end{frame}

\subsubsection{Building a Parse Tree with Derivations}
\subsubsectiontableofcontentslide

\begin{frame}{Algorithm to Build a Parse Tree with Derivations}
	\begin{scriptsize}
	\begin{myalgorithm}
	\SetKwFunction{node}{node}
	\SetKwFunction{nodelabel}{label}
	\SetKwFunction{nodechild}{addChild}
	\Inputs{A sequence of tokens $T$. A grammar $G$ with the start symbol $s_0$.}
	\Output{A parse tree that corresponds to $T$ and $G$.}
	\Begin{
		$r$ \affect \node($s_0$, $T$) ;	$L$ \affect $[r]$ ; $input[r]$ \affect $T$ \;
		\While(\tcc*[f]{Leftmost derivation}){$L = [n].L'$}{
			$L$ \affect $L'$ \;
			\If{$\exists (\nodelabel(n) \bnfopo b) \in G | input[n]$ matches $b$}{
				\ForEach{$\alpha s \beta = b$}{
					$m$ \affect $\omega \in T | (input[\alpha]\;\omega\;input[\beta]) = input[n]$ \;
					$c$ \affect \node($s$) \;
					\nodechild($n$,$c$) \;
					\If{$s$ is nonterminal}{
						$L$ \affect $L.[c]$ \;
						$input[c]$ \affect $m$ \;
					}
				}
			}
		}
		\Return $r$
	}
	\end{myalgorithm}
	\end{scriptsize}
\end{frame}

\begin{frame}[t]{Example of Parse Tree Building}
	\begin{columns}
		\begin{column}[t]{.7\linewidth}
			Let the grammar: \\[-1cm]
			\begin{small}
			\begin{bnf}
				\bnfprod*{E}{\bnfpn{E} \bnfts{+} \bnfpn{E}} \\
				\bnfalt*{\bnfpn{E} \bnfts{*} \bnfpn{E}} \\
				\bnfalt*{\bnfts{-} \bnfpn{E}} \\
				\bnfalt*{\bnfts{(} \bnfpn{E} \bnfts{)}} \\
				\bnfalt*{\bnfts{id}}
			\end{bnf}
			\end{small} \\[1em]
			Tokens: {\small $\bnfts{id}\bnfts{+}\bnfts{id}\bnfts{*}\bnfts{id}$} \\[1em]
			\begin{small}
			\only<1>{$L = [E]$}
			\only<2>{$L = [n].L' = [E]$ \\
				$input = \bnfts{id}\bnfts{+}\bnfts{id}\bnfts{*}\bnfts{id}$ \\
				$b = E_0 \bnfts{+} E_1$ \\
				$input_{E_0} = \bnfts{id}$ \\
				$input_{E_1} = \bnfts{id}\bnfts{*}\bnfts{id}$ \\
				$L = [E_0, E_1]$
			}
			\only<3,5>{$L = [n].L' = [E, E]$ \\
				$input = \bnfts{id}$ \\
				$b = \bnfts{id}$ \\
				$L = [E]$
			}
			\only<4>{$L = [n].L' = [E]$ \\
				$input = \bnfts{id}\bnfts{*}\bnfts{id}$ \\
				$b = E_0 \bnfts{*} E_1$ \\
				$input_{E_0} = \bnfts{id}$ \\
				$input_{E_1} = \bnfts{id}$ \\
				$L = [E_0, E_1]$
			}
			\only<6>{$L = [n].L' = [E]$ \\
				$input = \bnfts{id}$ \\
				$b = \bnfts{id}$ \\
				$L = []$
			}
			\end{small}
		\end{column}
		\begin{column}[t]{.3\linewidth}
			Parse tree is: \\[1em]
			\includeanimatedfigure[width=\linewidth]{parse_tree_example}
		\end{column}
	\end{columns}
\end{frame}

\subsection{Ambiguity of a grammar}
\subsubsectiontableofcontentslide

\sidecite{Cantor.1962, Floyd.1962}
\begin{frame}{{What is an} Ambiguous Grammar?}
	\begin{small}
	\alertbox*{A grammar that produces more than one parse tree for some sentence is said to be ambiguous}
	\begin{itemize}
	\item An ambiguous grammar is one that produces more than one leftmost derivation or more than one rightmost derivation for the same sentence
	\end{itemize}
	\begin{example}
		Leftmost derivations for the arithmetic expression \bnfts{id}\bnfts{+}\bnfts{id}\bnfts{*}\bnfts{id}
	\end{example}
	\end{small}
	\begin{tiny}
	\begin{tabular*}{\linewidth}{@{}c@{}|@{}c@{}}
		\begin{tabular}{lcl}
			$\bnfpn{E}$ & \deriv & $\bnfpn{E} \bnfts{+} \bnfpn{E}$ \\
			& \deriv & $\bnfts{id} \bnfts{+} \bnfpn{E}$ \\
			& \deriv & $\bnfts{id} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$ \\
			& \deriv & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfpn{E}$ \\
			& \deriv & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfts{id}$
		\end{tabular}
		\raisebox{-.5\height}{\includegraphics[width=.25\linewidth]{ambiguity_2}}
	&
		\begin{tabular}{lcll}
				$\bnfpn{E}$ & \deriv & $\bnfpn{E} \bnfts{*} \bnfpn{E}$ \\
				  & \deriv & $\bnfpn{E} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$ \\
				  & \deriv & $\bnfts{id} \bnfts{+} \bnfpn{E} \bnfts{*} \bnfpn{E}$ \\
				  & \deriv & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfpn{E}$ \\
				  & \deriv & $\bnfts{id} \bnfts{+} \bnfts{id} \bnfts{*} \bnfts{id}$
		\end{tabular}
		\raisebox{-.5\height}{\includegraphics[width=.25\linewidth]{ambiguity_1}}
	\end{tabular*}
	\end{tiny}
\end{frame}

\begin{frame}[background=6]{Grammar Ambiguity is not Desirable}
	\begin{itemize}
	\item For parsers, it is desirable that the grammar be made unambiguous. Otherwise we cannot determine which parse tree to select for a sentence
	\vfill
	\item Another way is to use carefully chosen ambiguous grammars, together with \emph{disambiguating rules} that discard undesirable parse trees, leaving only one tree for each sentence
	\end{itemize}
\end{frame}

\subsection{Verifying the language supported by a grammar}
\subsectiontableofcontentslide

\begin{frame}[t]{Verifying the Language Supported by a Grammar}
	\hiconbox{Even if compiler designers rarely do this task, it is useful to be able to verify if a language can be generated from a grammar}{information-icon}
	\hiconbox{Proof that a grammar $G$ generates a language $L$ has two parts:
		\begin{enumerate}
			\item Show up that every string generated by $G$ is in $L$
			\item Show up that every string in $L$ can be generated by $G$
	\end{enumerate}}{method-icon}
	\begin{example}
		\begin{small}
		Considerer the following grammar:
		\begin{center}$\bnfpn{S} \bnfopo \bnfts{(} \bnfpn{S} \bnfts{)} \bnfpn{S} \bnfor \bnfes$\end{center}
		It may not be apparent, but this grammar generates all the strings of balanced parentheses, and only such strings.  That why, we need to proceed the two steps of the proof
		\end{small}
	\end{example}
\end{frame}

\begin{frame}{{Part 1:} every string generated by $G$ is in $L$}
	\begin{myalgorithm}
		\SetKwInOut{Basis}{Basis}
		\SetKwInOut{Assumption}{Assumption}
		\SetKwInOut{Induction}{Induction}
		\Basis{The basis is $n=1$. The only string of terminals derivable from $\bnfpn{S}$ in one step is the empty string, which is balanced}
		\Assumption{
			Assume that all derivations of fewer than $n$ steps produce balanced sentences, and consider a leftmost derivation of exactly $n$ steps
		}
		\Induction{
			Such derivations must be of the form:
			\[
				\bnfpn{S} \derivlm \bnfts{(} \bnfpn{S} \bnfts{)} \bnfpn{S} \derivlm \bnfts{(} \alpha \bnfts{)} \bnfpn{S} \derivlm \bnfts{(} \alpha \bnfts{)} \beta
			\]
			Derivations of $\alpha$ and $\beta$ from $\bnfpn{S}$ take fewer than $n$ steps, so by the inductive hypothesis $\alpha$ and $\beta$ are balanced \\
			Therefore, the string ``$\bnfts{(}\alpha\beta$'' must be balanced
		}
	\end{myalgorithm}
\end{frame}

\begin{frame}[t]{Part 2: every string in $L$ can be generated by $G$}
	\begin{myalgorithm}
		\SetKwInOut{Basis}{Basis}
		\SetKwInOut{Induction}{Induction}
		\Basis{If the string has length $0$, it must be $\bnfes$, which is balanced}
		\Induction{
			\begin{itemize}
				\item Observe that every balanced string has even length
				\item Assume that every balanced string of length less than $2n$ is derivable from $\bnfpn{S}$, and consider a balanced string $w$ of length $2n$, $n\ge1$. Surely $w$ begins with a left parenthesis
				\item Let $\bnfts{(}\alpha\bnfts{)}$ be the shortest nonempty prefix of $w$ having an equal number of left and right parentheses
				\item Then $w$ can be written $w = \bnfts{(}\alpha\bnfts{)}\beta$ where both $\alpha$ and $\beta$ are balanced. Since $\alpha$ and $\beta$ are of length less than $2n$, they are derivable from $\bnfpn{S}$ by the inductive hypothesis. Thus, we can find a derivation of the form:
				\[
				\bnfpn{S} \deriv \bnfts{(} \bnfpn{S} \bnfts{)} \bnfpn{S} \seqderiv \bnfts{(} \alpha \bnfts{)} \bnfpn{S} \seqderiv \bnfts{(} \alpha \bnfts{)} \beta
				\]
				\item Proving that $w = \bnfts{(}\alpha\bnfts{)}\beta$ is also derivable from $\bnfpn{S}$
			\end{itemize}
		}
	\end{myalgorithm}
\end{frame}

\subsection{Context-free grammar and regular expression}
\subsectiontableofcontentslide

\begin{frame}[t]{{Context-Free Grammar} v.s. Regular Expression}
	\hiconbox{
		Grammars are more powerful notation than regular expressions \newline
		Every construct that can be described by a regular expression can be described by a grammar, but not vice-versa}{information-icon}
	\vspace{-.5cm}
	\begin{columns}
		\begin{column}[t]{.5\linewidth}
			\begin{example}[Grammar, and Regex]
				Regex \regex{(a\ensuremath{|}b)*abb} and the following grammar describe the same language:
				\begin{small}
					\begin{bnf}
						\bnfprod*{A}{\bnfts{a} \bnfpn{A}} \\
						\bnfalt*{\bnfts{b} \bnfpn{A}} \\
						\bnfalt*{\bnfts{a} \bnfpn{B}} \\
						\bnfprod*{B}{\bnfts{b} \bnfpn{C}} \\
						\bnfprod*{C}{\bnfts{b} \bnfpn{D}} \\
						\bnfprod*{D}{\bnfes}
					\end{bnf}
				\end{small}
			\end{example}
		\end{column}
		\begin{column}[t]{.5\linewidth}
			\begin{example}[Grammar, no Regex]
				Language $L = { a^nb^n | n \ge 1 }$ can be described by a grammar but not by a regular expression (except with Posix extension)
			\end{example}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[t]{From Nondeterministic Finite Automaton to Grammar}
	\begin{footnotesize}
	\begin{myalgorithm}
		\SetKwFunction{createNonterminal}{createNonterminal}
		\SetKwFunction{makeFirstProduction}{makeFirstProduction}
		\Begin{
			\ForEach{state $i$ of the NFA}{
				$A_i$ \affect \createNonterminal($i$) \;
				\ForEach{transition $t$ from $i$ to $j$}{
					\If{$t$ on token $\bnfts{a}$}{
						$P$ \affect $P \cup \left(\bnfpn{\ensuremath{A_i}} \bnfopo \bnfts{a}\bnfpn{\ensuremath{A_j}}\right)$ \;
					}
					\If{$t$ on $\bnfes$}{
						$P$ \affect $P \cup \left(\bnfpn{\ensuremath{A_i}} \bnfopo \bnfpn{\ensuremath{A_j}}\right)$ \;
					}
				}
				\If{$i$ is accepting state}{
					$P$ \affect $P \cup \left(\bnfpn{\ensuremath{A_i}} \bnfopo \bnfes\right)$ \;
				}
				\If{$i$ is starting state}{
					\makeFirstProduction{$A_i$} \;
				}
			}
		}
	\end{myalgorithm}
	\end{footnotesize}
\end{frame}

\begin{frame}[t]{Why using Regex?}
	\begin{alertblock}{Why use regular expressions to define the lexical syntax of a language?}
		\begin{enumerate}
			\item Separating the syntactic structure of a language into lexical and non-lexical parts provides a better modularity
			\item Lexical rules of a language are frequently quite simple, and to describe them we do not need a notation as complex as the grammars
			\item Regular expressions generally provide a more concise and easier-to-understand notation for tokens than grammars
			\item More efficient lexical analyzers can be constructed automatically from regular expressions than from arbitrary grammars
		\end{enumerate}
	\end{alertblock}
	\simplebox*[.45\linewidth]{Regular expressions are useful to describe constructs such as identifiers, numbers\dots}
	\hfill
	\simplebox*[.45\linewidth]{Grammars are most useful for describing nested structures such as balanced parentheses, corresponding if-then-else\dots}
\end{frame}

\subsection{Optimizing the Grammar}
\subsubsectiontableofcontentslide*

\subsubsection{Eliminating the ambiguity}

\begin{frame}[t]{{Example of Ambiguity:} ``dangling else''}
	\begin{small}
	\alertbox{An ambiguous grammar can be rewritten to eliminate the ambiguity}
	\begin{description}
	\item[Grammar]
			\begin{bnf}
				\bnfprod*{statement}{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement}} \\
				\bnfalt*{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement} \bnfts{else} \bnfpn{statement}} \\
				\bnfalt*{\bnfts{other}}
			\end{bnf}
	\item[Input] \bnfts{if} $E_1$ \bnfts{then} \bnfts{if} $E_2$ \bnfts{then} $S_1$ \bnfts{else} $S_2$
	\end{description}
	\pgfuseimage{ambiguity-3}\hfill
	\pgfuseimage{ambiguity-4}
	\begin{itemize}
	\item The first tree is preferred according to ``Match each \bnfts{else} with the closest unmatched \bnfts{then}.'' This rule is rarely built into productions
	\end{itemize}
	\end{small}
\end{frame}

\begin{frame}[t]{{Solving Ambiguity:} ``dangling else'' --- Complex}
	\begin{footnotesize}
	The disambiguation of this ``if-then-else'' problem may be included into a new grammar
	\begin{bnf}
		\bnfprod*{statement}{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement}} \\
		\bnfalt*{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement} \bnfts{else} \bnfpn{statement}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}
	\begin{center}\pgfuseimage{bottom-arrow}\end{center}
	\begin{bnf}
		\bnfprod*{statement}{\bnfpn{matched\_statement}} \\
		\bnfalt*{\bnfpn{open\_statement}} \\
		\bnfprod*{matched\_statement}{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{matched\_statement}} \\
		\bnfmore*{\bnfts{else} \bnfpn{matched\_statement}} \\
		\bnfalt*{\bnfts{id}} \\
		\bnfprod*{open\_statement}{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement}} \\
		\bnfalt*{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{matched\_statement} \bnfts{else} \bnfpn{open\_statement}}
	\end{bnf}
	\end{footnotesize}
\end{frame}

\begin{frame}[t]{{Solving Ambiguity:} ``dangling else'' --- Simple}
	\begin{footnotesize}
	The disambiguation of this ``if-then-else'' problem may be included into a new grammar
	\begin{bnf}
		\bnfprod*{statement}{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement}} \\
		\bnfalt*{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement} \bnfts{else} \bnfpn{statement}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}
	\begin{center}\pgfuseimage{bottom-arrow}\end{center}
	\begin{bnf}
		\bnfprod*{statement}{\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement} \bnfpn{else\_statement}} \\
		\bnfalt*{\bnfts{id}} \\
		\bnfprod*{else\_statement}{\bnfts{else} \bnfpn{statement}} \\
		\bnfalt*{\bnfes}
	\end{bnf}
	\end{footnotesize}
\end{frame}

\subsubsection{Eliminating the left recursion}
\subsubsectiontableofcontentslide

\begin{frame}{{Left Recursive} Grammar}
	\begin{definitionblock}{Left-Recursive Grammar}
		Grammar is left recursive if it has a nonterminal $\bnfpn{A}$ such that for some string $\alpha$ there is a derivation \[ \bnfpn{A} \seqderivone \bnfpn{A}\alpha \]
	\end{definitionblock}
	\vspace{1cm}
	\hiconbox{Top-down parsing methods cannot handle left-recursive grammars}{information-icon}
	\vspace{1cm}
	\alertbox*{A transformation is needed to eliminate left recursion}
\end{frame}

\begin{frame}{Example of Eliminating Left Recursion}
	\begin{minipage}{.4\linewidth}
		\vspace{-1cm}
		\begin{bnf}
			\bnfprod*{E}{\bnfpn{E} \; \bnfts{+} \; \bnfpn{E}} \\
			\bnfalt*{\bnfpn{E} \; \bnfts{*} \; \bnfpn{E}} \\
			\bnfalt*{\bnfts{-} \; \bnfpn{E}} \\
			\bnfalt*{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
			\bnfalt*{\bnfts{id}}
		\end{bnf}
	\end{minipage}
	\pgfuseimage{right-arrow}
	\begin{minipage}{.4\linewidth}
		\vspace{-1cm}
		\begin{bnf}
			\bnfprod*{E}{\bnfts{-} \; \bnfpn{E} \; \bnfpn{\ensuremath{R_E}}} \\
			\bnfalt*{\bnfts{(} \; \bnfpn{E} \; \bnfts{)} \; \bnfpn{\ensuremath{R_E}}} \\
			\bnfalt*{\bnfts{id} \; \bnfpn{\ensuremath{R_E}}} \\
			\bnfprod*{\ensuremath{R_E}}{\bnfts{+} \; \bnfpn{E} \; \bnfpn{\ensuremath{R_E}}} \\
			\bnfalt*{\bnfts{*} \; \bnfpn{E} \; \bnfpn{\ensuremath{R_E}}} \\
			\bnfalt*{\bnfes}
		\end{bnf}
	\end{minipage}
\end{frame}

\begin{frame}[t]{Eliminating Left Recursion}
	\begin{myalgorithm}
	\Input{Grammar $G = \langle N, \Sigma, P, S\rangle$}
	\Output{An equivalent grammar without left recursion}
	\BlankLine
	\Begin{
		\While{$\exists A | (\bnfpn{A} \bnfopo \bnfpn{A}\;\gamma) \in P$}{
			\ForEach{$p = (\bnfpn{A} \bnfopo b\;\delta) \in P \wedge b \neq A$}{
				$P \affect P \setminus \{ p \}$ \;
				$P \affect P \cup \left\{ (\bnfpn{\ensuremath{R_A}} \bnfopo b\;\delta\;\bnfpn{\ensuremath{R_A}}) \right\}$ \;
			}
			$P = P \cup \left\{ (\bnfpn{\ensuremath{R_A}} \bnfopo \bnfes) \right\}$ \;
			\ForEach{$p = (\bnfpn{A} \bnfopo \bnfpn{A}\;\omega) \in P$}{
				$P \affect P \setminus \{ p \}$ \;
				$P \affect P \cup \left\{ (\bnfpn{A} \bnfopo \omega\;\bnfpn{\ensuremath{R_A}}) \right\}$ \;
			}
		}
	}
	\end{myalgorithm}
\end{frame}

\subsubsection{Left factoring}
\subsubsectiontableofcontentslide

\begin{frame}{Left Factoring}
	\hiconbox{When the choice between two alternatives $A$-productions is not clear, we may be able to rewrite the productions to defer the decision until enough of the input has been seen that we can make the right choice}{information-icon}
	\vspace{.5cm}
	\begin{bnf}
		\bnfprod*{statement}{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{expression} \; \bnfts{else} \; \bnfpn{expression}} \\
		\bnfalt*{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{expression}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}
	\vspace{.5cm}
	\alertbox*{Left factoring is a grammar transformation that is useful for producing a grammar suitable for predictive, or top-down, parsing}
\end{frame}

\begin{frame}{Example of Left Factoring}
	\begin{bnf}
		\bnfprod*{statement}{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{statement} \; \bnfts{else} \; \bnfpn{statement}} \\
		\bnfalt*{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{statement}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}
	\begin{center}\pgfuseimage{bottom-arrow}\end{center}
	\begin{bnf}
		\bnfprod*{statement}{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{statement} \; \bnfpn{else\_statement}} \\
		\bnfalt*{\bnfts{id}} \\
		\bnfprod*{else\_statement}{\bnfts{else} \; \bnfpn{statement}} \\
		\bnfalt*{\bnfes}
	\end{bnf}
\end{frame}

\begin{frame}{Algorithm for Left Factoring}
	\begin{myalgorithm}
	\Input{Grammar $G = \langle N, \Sigma, P, S\rangle$}
	\Output{An equivalent left-factored grammar}
	\BlankLine
	\Begin{
		\While{$\exists A  \in P | (\bnfpn{A} \bnfopo \alpha\;\gamma), (\bnfpn{A} \bnfopo \alpha\;\delta)$}{
			\ForEach{$p = (\bnfpn{A} \bnfopo \alpha\;\omega) \in P$}{
				$P \affect P \setminus \{ p \}$ \;
				\If{$\omega \neq \epsilon$}{
					$P \affect P \cup \left\{ (\bnfpn{\ensuremath{R_A}} \bnfopo \omega) \right\}$ \;
				}
			}
			$P \affect P \cup \left\{ (\bnfpn{A} \bnfopo \alpha\;\bnfpn{\ensuremath{R_A}}) \right\}$ \;
			$P \affect P \cup \left\{ (\bnfpn{\ensuremath{R_A}} \bnfopo \bnfes) \right\}$ \;
		}
	}
	\end{myalgorithm}
\end{frame}

\section{Parsing with a grammar}
\sectiontableofcontentslide*

\subsection{FIRST and FOLLOW functions}

\begin{frame}[background=6]{{What are the Functions} FIRST and FOLLOW?}
	\begin{itemize}
	\item The construction of both top-down and bottom-up parsers is aided by two functions associated with a grammar G:
		\begin{enumerate}
		\item FIRST
		\item FOLLOW
		\end{enumerate}
	\vfill
	\item These functions allow us to choose which production to apply, based on the next input symbol
	\vfill
	\item During panic-mode error recovery the set of tokens replied by FOLLOW can be used as synchronizing tokens
	\end{itemize}
\end{frame}

\begin{frame}{Definition of FIRST}
	\begin{itemize}
	\item Define FIRST($\alpha$), where $\alpha$ is any string of grammar symbols, to be the set of terminals that begin strings derived from $\alpha$
	\item If $\alpha \seqderiv \bnfes$, then $\bnfes$ is also in FIRST($\alpha$)
	\end{itemize}
	\vfill
	\begin{example}
		\begin{columns}
			\begin{column}[t]{.4\linewidth}
				$A \seqderiv c \; \gamma$ \\
				FIRST($A$) $ = \{ c \}$
			\end{column}
			\begin{column}[t]{.4\linewidth}
				\raisebox{-\height}{\includegraphicswtex[width=\linewidth]{example_FIRST}}
			\end{column}
		\end{columns}
	\end{example}
\end{frame}

\begin{frame}[background=8]{Algorithm of FIRST}
	\begin{itemize}
	\item To compute FIRST($X$), apply the following rules until no more terminals or $\bnfes$ can be added to any FIRST set
	\vfill
		\begin{enumerate}
		\item FIRST($X$) $= \{ X \}$ if $X$ is a terminal
		\item If $X$ is a nonterminal and $\bnfpn{X} \bnfopo Y_1 \; Y_2 \dots Y_k$ is a production for some $k \ge 1$, then 
			\begin{itemize}
				\item Add $a$ in FIRST($X$) if $\exists i$ such that $a \in$ FIRST($Y_i$) 
				\item Add $\bnfes$ in FIRST($X$) if $Y_1\;Y_2 \dots Y_k \seqderiv \epsilon$
				\item Add $\bnfes$ to FIRST($X$) if $\forall j \in \{1, 2, \dots, k\}$, $\bnfes \in$ FIRST($Y_j$)
			\end{itemize}
		\item Add $\bnfes$ to FIRST($X$) if $\bnfpn{X} \bnfopo \bnfes$ is a production
		\end{enumerate}
	\vfill
	\item Add all non-$\bnfes$ symbols of FIRST($X_i$) for $i \in \{1 \dots n\}$ to FIRST($X_1\;X_2 \dots X_n$) 
	\end{itemize}
\end{frame}

\begin{frame}{Definition of FOLLOW}
	\begin{scriptsize}
	\begin{itemize}
	\item Define FOLLOW($A$), where $A$ is a nonterminal, to be the set of terminals $a$ that can appear immediately to the right of $A$ in some sentential form
	\item The set of terminals $a$ such that there exists a derivation of the form $S \seqderiv \alpha\;A\;a\;\beta$, for some $\alpha$ and $\beta$
	\item Note that there may have been symbols between $A$ and $a$, at some time during the derivation, but if so, they derive $\bnfes$ and disappeared
	\item If $A$ can be the rightmost symbol, then \bnfts{eof} (or usually \bnfts{\$}) is in FOLLOW($A$)
	\end{itemize}
	\end{scriptsize}
	\vfill
	\begin{small}
	\begin{example}
		\begin{columns}
			\begin{column}[t]{.4\linewidth}
				$A \seqderiv c\;\gamma$ \\
				$a \in $ FOLLOW($A$)
			\end{column}
			\begin{column}[t]{.4\linewidth}
				\raisebox{-\height}{\includegraphicswtex[width=\linewidth]{example_FIRST}}
			\end{column}
		\end{columns}
	\end{example}
	\end{small}
\end{frame}

\begin{frame}[background=8]{Algorithm of FOLLOW}
	\begin{itemize}
	\item To compute FOLLOW($A$) for a nonterminal $A$, apply the following rules until nothing can be added to any FOLLOW set
		\begin{enumerate}
		\vfill
		\item Place \bnfts{eof} in FOLLOW($S$), where $S$ is the start symbol, and \bnfts{eof} is the input right end-marker
		\vfill
		\item If there is a production $\bnfpn{A} \bnfopo \alpha\;\bnfpn{B}\;\beta$, then everything in FIRST($\beta$), except $\bnfes$ is added in FOLLOW($B$)
		\vfill
		\item If there is a production $\bnfpn{A} \bnfopo \alpha\;\bnfpn{B}$, or a production $\bnfpn{A} \bnfopo \alpha\;\bnfpn{B}\;\beta$, where FIRST($\beta$) contains $\bnfes$, then everything in FOLLOW($A$) is added to FOLLOW($B$)
		\end{enumerate}
	\end{itemize}
\end{frame}

\subsection{Top-down parsing}

\subsubsection{Principles}
\subsubsectiontableofcontentslide*

\begin{frame}[t]{Top-Down Parsing}
	\begin{definitionblock}{Top-Down Parsing}
		Constructing a parse tree for the input string, starting from the root of the grammar, and creating the nodes of the parse tree in preorder
	\end{definitionblock}
	\vspace{.25cm}
	\alertbox*{Top-down parsing can be viewed as finding a leftmost derivation for an input string}
	\vspace{.25cm}
	\begin{block}{Illustrative Input String and Grammar}
		\Emph{Input string:} $\bnfts{id}\bnfts{+}\bnfts{id}\bnfts{*}\bnfts{id}$ \\
		\Emph{Grammar:}
		\vspace{-1cm}
		\begin{bnf}\smaller\smaller
			\bnfprod*{E}{\bnfpn{T} \; \bnfpn{E'}} \\
			\bnfprod*{E'}{\bnfts{+} \; \bnfpn{T} \; \bnfpn{E'} \bnfor \bnfes} \\
			\bnfprod*{T}{\bnfpn{F} \; \bnfpn{T'}} \\
			\bnfprod*{T'}{\bnfts{*} \; \bnfpn{F} \; \bnfpn{T'} \bnfor \bnfes} \\
			\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)} \bnfor \bnfts{id}}
		\end{bnf}
	\end{block}
\end{frame}

\begin{frame}{Types of Top-Down Parsing}
	\begin{center}
	\fancybox{Recursive-descent Parsing}{General form, which may require backtracking to find the correct $A$-production}{descent-icon}{}
	\hspace{2cm}
	\fancybox{Predictive Parsing}{Special form without backtracking; $A$-production chosen by looking ahead a fixed number of tokens}{predictive-icon}{}
	\end{center}
	\alertbox*{Class of grammars dedicated to the predictive parsers looking $k$ symbols ahead in the input is called $LL(k)$ class}
\end{frame}

\subsubsection{Recursive-descent parsing}
\subsubsectiontableofcontentslide

\sidecite{Hoare.1962, Schorre.1964, McClure.1965}
\begin{frame}{Recursive-Descent Parsing}
	\simplebox{Recursive-descent parsing program consists of a set of procedures, one for each nonterminal}
	\vspace{.25cm}
	\begin{myprocedure}{$A$}{}\smaller
		\SetKwFunction{call}{call}
		\Input{Production $\bnfpn{A} \bnfopo \alpha_1\dots\alpha_k$}
		\Begin{
			\For{$i$ \affect $1$ \KwTo $k$}{
				\uIf{$\alpha_i$ is nonterminal}{
					\call $\alpha_i$() \;
				}
				\uElseIf{$\alpha_i = $current input symbol $a$}{
					forward \affect forward + 1 \tcp*[f]{Move input pointer}\;
				}
				\Else{
					Report an error \;
				}
			}
		}
	\end{myprocedure}
\end{frame}

\begin{frame}{Problem of Infinite Loop}
	\alertbox{
		Left-recursive grammar can cause a recursive-descent \\
		parser to go into an infinite loop
	}
	\begin{itemize}
	\item \Emph{When:} expand a nonterminal $A$, the same nonterminal is found again and expanded without consuming input
	\end{itemize}
	\vfill
	\begin{columns}
		\begin{column}{.5\linewidth}
			\begin{bnf}
				\bnfprod*{E}{\bnfpn{E} \; \bnfts{+} \; \bnfpn{E}} \\
				\bnfalt*{\bnfpn{E} \; \bnfts{*} \; \bnfpn{E}} \\
				\bnfalt*{\bnfts{-} \; \bnfpn{E}} \\
				\bnfalt*{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
				\bnfalt*{\bnfts{id}}
			\end{bnf}
		\end{column}
		\begin{column}{.5\linewidth}
			\begin{tabular}{@{}ll@{}}
				E & \deriv E \bnfts+ E \\
				& \deriv E \bnfts+ E \bnfts+ E \\
				& \deriv E \bnfts+ E \bnfts+ E \bnfts+ E \\
				& \deriv E \bnfts+ E \bnfts+ E \bnfts+ E \bnfts+ E \\
				& \deriv \dots
			\end{tabular}
		\end{column}
	\end{columns}	
\end{frame}

\subsubsection{$LL(1)$ grammars}
\subsubsectiontableofcontentslide

\sidecite{Lewis.1968, Birman.1973}
\begin{frame}{$LL(1)$ Grammar}
	\begin{definitionblock}{$LL(1)$ Grammar}
		Class of grammars that have the following properties:
		\begin{itemize}
			\item {\Large\Emph{L}}eft-to-right input scanning
			\item {\Large\Emph{L}}eftmost derivation
			\item {\Large\Emph{1}} input symbol is used for lookahead to make parsing action decisions
		\end{itemize}
	\end{definitionblock}
	\begin{itemize}
	\item Predictive parser can be constructed for $LL(1)$ grammars, because no backtracking is needed
	\vfill
	\item $LL(1)$ grammars are rich enough to cover most programming constructs
	\item But, they must be neither left-recursive nor ambiguous
	\end{itemize}
\end{frame}

\begin{frame}[t]{Definition of a $LL(1)$ Grammar}
	\begin{small}
		\begin{definitionblock}{$LL(1)$ Grammar (refined)}
			Grammar $G$ is $LL(1)$ iff whenever $\bnfpn{A} \bnfopo \alpha \bnfor \beta$ are two distinct productions of $G$, the following conditions hold:
				\begin{enumerate}
				\item For nonterminal $\bnfts{a}$, both $\alpha$ and $\beta$ derive strings beginning with $\bnfts{a}$
				\item At most one of $\alpha$ and $\beta$ can derive the empty string
				\item If $\beta\seqderiv\bnfes$, then $\alpha$ does not derive any string beginning with a terminal in FOLLOW($A$)
				\item Likewise, if $\alpha\seqderiv\bnfes$, then $\beta$ does not derive any string beginning with a terminal in FOLLOW($A$)
				\end{enumerate}
		\end{definitionblock}
		\vspace{-.6cm}
		\begin{bnf}
			\bnfprod*{statement\_list}{\bnfpn{statement} \; \bnfpn{statement\_list}} \\
			\bnfalt*{\bnfes} \\
			\bnfprod*{statement}{\bnfts{if} \bnfts{(} \bnfpn{expression} \bnfts{)} \bnfpn{statement} \bnfts{else} \bnfpn{statement}} \\
			\bnfalt*{\bnfts{while} \bnfts{(} \bnfpn{expression} \bnfts{)} \bnfpn{statement}} \\
			\bnfalt*{\bnfts{\{} \bnfpn{statement\_list} \bnfts{\}}}
		\end{bnf}
	\end{small}
\end{frame}

\begin{leftlawnframe}{Parsing a $LL(1)$ Grammar}{ll1}
		To parse an input string, a \Emph{table} should be build \\[.2cm]
		It determines the production to use from a given production and the input symbol \\[.2cm]
		Table $M[A,a]$ is built from FIRST and FOLLOW sets, where $A$ is a nonterminal, and $a$ is a terminal or \bnfts{eof}
\end{leftlawnframe}

\begin{frame}[t]{{Algorithm} for Building the Predictive-Parsing Table}
	\vspace{-.25cm}
	\begin{myalgorithm}\smaller
		\Input{Grammar $G = \langle N, \Sigma, P, S\rangle$}
		\Output{Parsing table $M$}
		\BlankLine
		\Begin{
			\ForEach{$\left(\bnfpn{A} \bnfopo \alpha\right) \in P$}{
				\ForEach{terminal $a \in$ FIRST($\alpha$)}{
					$M[A,a]$ \affect $M[A,a] \cup \left(\bnfpn{A} \bnfopo \alpha\right)$
				}
				\If{$\bnfes \in$ FIRST($\alpha$)}{
					\ForEach{$b \in$ FOLLOW($A$)}{
						$M[A,b]$ \affect $M[A,b] \cup \left(\bnfpn{A} \bnfopo \alpha\right)$
					}
					\If{\bnfts{eof} $\in$ FOLLOW($A$)}{
						$M[A,\bnfts{eof}]$ \affect $M[A,\bnfts{eof}] \cup \left(\bnfpn{A} \bnfopo \alpha\right)$
					}
				}
			}
			\If{$\forall \alpha$, $M[A,\alpha] = \emptyset$}{
				$M[A,\alpha]$ \affect error
			}
		}
	\end{myalgorithm}
\end{frame}

\begin{frame}[t]{Example of Table Building}
	\begin{scriptsize}
		\begin{minipage}{.25\linewidth}
			\begin{bnf}
				\bnfprod*{E}{\bnfpn{T} \; \bnfpn{E'}} \\
				\bnfprod*{E'}{\bnfts{+} \; \bnfpn{T} \; \bnfpn{E'}} \\
				\bnfalt*{\bnfes} \\
				\bnfprod*{T}{\bnfpn{F} \; \bnfpn{T'}} \\
				\bnfprod*{T'}{\bnfts{*} \; \bnfpn{F} \; \bnfpn{T'}} \\
				\bnfalt*{\bnfes} \\
				\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
				\bnfalt*{\bnfts{id}}
			\end{bnf}
		\end{minipage}
	\end{scriptsize}
	\begin{small}
	\mbox{}\vspace{.5cm}
	\begin{tabularx}{\linewidth}{|c|X|X|X|X|X|X|}
		\hline
		\tabularheading&\chead{$\bnfts{id}$}&\chead{$\bnfts{+}$}&\chead{$\bnfts{*}$}&\chead{$\bnfts{(}$}&\chead{$\bnfts{)}$}&\chead{\bnfts{eof}} \\
		\hline
		$E$ & \only<3->{\bnfmark{1}} & & & \only<3->{\bnfmark{1}} & & \\
		\hline
		$E'$ & & \only<4->{\bnfmark{2}} & & & \only<5->{\bnfmark{3}} & \only<6->{\bnfmark{3}} \\
		\hline
		$T$ & \only<7->{\bnfmark{4}} & & & \only<7->{\bnfmark{4}} & & \\
		\hline
		$T'$ & & \only<9->{\bnfmark{6}} & \only<8->{\bnfmark{5}} & & \only<9->{\bnfmark{6}} & \only<9->{\bnfmark{6}} \\
		\hline
		$F$ & \only<11->{\bnfmark{8}} & & & \only<10->{\bnfmark{7}} & & \\
		\hline
	\end{tabularx}
	\end{small}

	\begin{scriptsize}
	\putat(150,200){\parbox[t]{.55\paperwidth}{\normalfont\normalcolor\small
		\only<2-3>{	\Emph{For production:} \bnfmark{1}~$\bnfpn{E} \bnfopo \bnfpn{T} \bnfpn{E'}$ \\
				FIRST($T E'$) = FIRST($T$) = FIRST($F$) = $\{ \bnfts{(}, \bnfts{id} \}$ \\
				\only<3>{Then put the production in $M[E,\bnfts(]$ and $M[E,\bnfts{id}]$; the rest of the line is \emph{error}}
		}
		\only<4>{\Emph{For production:} \bnfmark{2}~$\bnfpn{E'} \bnfopo \bnfts{+} \bnfpn{T} \bnfpn{E'}$ \\
				FIRST($\bnfts{+} T E'$) = $\{ \bnfts{+} \}$ \\
				Then put the production in $M[E',\bnfts{+}]$
		}
		\only<5,6>{	\Emph{For production:} \bnfmark{3}~$\bnfpn{E'} \bnfopo \bnfes$ \\
				FIRST($\bnfes$) = $\{ \bnfes \}$ \\
				FOLLOW($E'$) = $\{ \bnfts{)}, \bnfts{eof} \}$ \\
				Put the production in $M[E',\bnfts)]$ \\
				\only<6>{Put the production in $M[E',\bnfts{eof}]$}
		}
		\only<7>{	\Emph{For production:} \bnfmark{4}~$\bnfpn{T} \bnfopo \bnfpn{F} \bnfpn{T'}$ \\
				FIRST($F\;T'$) = FIRST($F$) = $\{ \bnfts{(}, \bnfts{id} \}$ \\
				Put the production in $M[T,\bnfts{(}]$ and $M[T,\bnfts{id}]$
		}
		\only<8>{	\Emph{For production:} \bnfmark{5}~$\bnfpn{T'} \bnfopo \bnfts{*} \bnfpn{F} \bnfpn{T'}$ \\
				FIRST($\bnfts{*}\;F\;T'$) = $\{ \bnfts{*} \}$ \\
				Put the production in $M[T',\bnfts{*}]$
		}
		\only<9>{	\Emph{For production:} \bnfmark{6}~$\bnfpn{T'} \bnfopo \bnfes$ \\
				FIRST($\bnfes$) = $\{ \bnfes \}$ \\
				FOLLOW($T'$) = $\{ \bnfts{+}, \bnfts{)}, \bnfts{eof} \}$ \\
				Put the production in $M[T',\bnfts{+}]$ and $M[T,\bnfts{)}]$ \\
				Put the production in $M[T',\bnfts{eof}]$
		}
		\only<10>{	\Emph{For production:} \bnfmark{7}~$\bnfpn{F} \bnfopo \bnfts{(} \bnfpn{E} \bnfts{)}$ \\
				FIRST($\bnfts{(}\;E\bnfts{)}$) = $\{ \bnfts{(} \}$ \\
				Put the production in $M[F,\bnfts{(}]$
		}
		\only<11>{	\Emph{For production:} \bnfmark{8}~$\bnfpn{F} \bnfopo \bnfts{id}$ \\
				FIRST($\bnfts{id}$) = $\{ \bnfts{id} \}$ \\
				Put the production in $M[F,\bnfts{id}]$
		}
	}}
	\end{scriptsize}
\end{frame}

\subsubsection{Nonrecursive predictive parsing}
\subsubsectiontableofcontentslide

\begin{frame}{Nonrecursive Predictive Parsing}
	\begin{itemize}
	\item Nonrecursive predictive parser can be built by maintaining a stack explicitly, rather than implicitly via recursive calls
	\item Parser mimics a leftmost derivation. If \code{w} is the input that has been matched so far, then the stack holds a sequence of grammar symbols a such that:
		\begin{center}
			$S \seqderiv w\;\alpha$
		\end{center}
	\item Table-driven parser has an \emph{input buffer}, a \emph{stack} containing a sequence of grammar symbols, a \emph{parsing table}, and an \emph{output stream}
	\end{itemize}
	\begin{center}
		\pgfuseimage{nonrecursive-predictive-parser}
	\end{center}
\end{frame}

\begin{frame}[t]{Algorithm of the Nonrecursive Predictive Parsing}
	\begin{scriptsize}
	\begin{myalgorithm}
	\SetKwFunction{nextInputSymbol}{nextInputSymbol}
	\SetKwFunction{pop}{pop}
	\SetKwFunction{push}{push}
	\SetKwFunction{topOf}{topOf}
	\SetKwFunction{print}{print}
	\Input{A string $w$, a parsing table $M$ for grammar $G$, and a start symbol $s_0$}
	\Output{If $w$ is in $L(G)$, a lef-most derivation of $w$; otherwise, an error indication}
	\BlankLine
	\Begin{
		$a$ \affect \nextInputSymbol\;
		\push($S$, \bnfts{eof}) \;
		\push($S$, $s_0$) \;
		\While{$\topOf(S) \neq \bnfts{eof}$}{
			$X$ \affect \topOf($S$) \;
			\uIf{$X = a$}{
				\pop($S$); $a$ \affect \nextInputSymbol\;
			}
			\uElseIf{$X$ is a terminal}{
				Report an error\;
			}
			\uElseIf{$M[X,a] = \left(\bnfpn{X} \bnfopo Y_1 \dots Y_n\right)$}{
				\print($\bnfpn{X} \bnfopo Y_1 \dots Y_n$)\;
				\pop($S$)\;
				\lFor{$i$ \affect $n$ \KwTo $1$}{\push($S, Y_i$)}
			}
			\Else{
				 Report an error\tcp*[f]{$M[X,a]$ is empty}\;
			}
		}
	}
	\end{myalgorithm}
	\end{scriptsize}
\end{frame}

\begin{frame}[t,fragile]{Example of Nonrecursive Predictive Parsing}
	\only<1>{\putat(-430,120){\pgfuseimage{rightarrow-in-code}}}
	\only<2>{\putat(-430,113){\pgfuseimage{rightarrow-in-code}}}
	\only<3>{\putat(-430,106){\pgfuseimage{rightarrow-in-code}}}
	\only<4,8,12,16,18,22>{\putat(-430,92){\pgfuseimage{rightarrow-in-code}}}
	\only<5,9,13,19,23>{\putat(-430,50){\pgfuseimage{rightarrow-in-code}}}
	\only<6,10,14,20,24>{\putat(-430,43){\pgfuseimage{rightarrow-in-code}}}
	\only<7,11,15,21,25>{\putat(-430,36){\pgfuseimage{rightarrow-in-code}}}
	\only<17>{\putat(-430,78){\pgfuseimage{rightarrow-in-code}}}
	\begin{tiny}
	\begin{columns}
		\begin{column}{.5\linewidth}
		\begin{myalgorithm}
		\SetKwFunction{nextInputSymbol}{nextInputSymbol}
		\SetKwFunction{pop}{pop}
		\SetKwFunction{push}{push}
		\SetKwFunction{topOf}{topOf}
		\SetKwFunction{print}{print}
		\Begin{
			$a$ \affect \nextInputSymbol\;
			\push($S$, \bnfts{eof}) \;
			\push($S$, $s_0$) \;
			\While{$\topOf(S) \neq \bnfts{eof}$}{
				$X$ \affect \topOf($S$) \;
				\uIf{$X = a$}{
					\pop($S$); $a$ \affect \nextInputSymbol\;
				}
				\uElseIf{$X$ is a terminal}{
					Report an error\;
				}
				\uElseIf{$M[X,a] = \left(\bnfpn{X} \bnfopo Y_1 \dots Y_n\right)$}{
					\print($\bnfpn{X} \bnfopo Y_1 \dots Y_n$)\;
					\pop($S$)\;
					\lFor{$i$ \affect $n$ \KwTo $1$}{\push($S, Y_i$)}
				}
				\Else{
					 Report an error\;
				}
			}
		}
		\end{myalgorithm}
		\end{column}
		\begin{column}[t]{.5\linewidth}
			\raisebox{-.5\height}{\includeanimatedfigure[width=\linewidth]{nonrecursive_predictive_parsing_example}}
		\end{column}
	\end{columns}
	\begin{tabularx}{.6\linewidth}[t]{|c|X|X|X|X|X|X|}
		\hline
		\tabularheading&\chead{$\bnfts{id}$}&\chead{$\bnfts{+}$}&\chead{$\bnfts{*}$}&\chead{$\bnfts{(}$}&\chead{$\bnfts{)}$}&\chead{\bnfts{eof}} \\
		\hline
		$E$ & \bnfmark{1} & & & \bnfmark{1} & & \\
		\hline
		$E'$ & & \bnfmark{2} & & & \bnfmark{3} & \bnfmark{3} \\
		\hline
		$T$ & \bnfmark{4} & & & \bnfmark{4} & & \\
		\hline
		$T'$ & & \bnfmark{6} & \bnfmark{5} & & \bnfmark{6} & \bnfmark{6} \\
		\hline
		$F$ & \bnfmark{8} & & & \bnfmark{7} & & \\
		\hline
	\end{tabularx}
	\end{tiny}
	\hfill
	\begin{tiny}
	\raisebox{-\height}{\begin{minipage}{.33\linewidth}
		\vspace{-.5cm}\begin{bnf}
		\bnfprod*{E}{\bnfpn{T} \; \bnfpn{E'}} \\
		\bnfprod*{E'}{\bnfts{+} \; \bnfpn{T} \; \bnfpn{E'}} \\
		\bnfalt*{\bnfes} \\
		\bnfprod*{T}{\bnfpn{F} \; \bnfpn{T'}} \\
		\bnfprod*{T'}{\bnfts{*} \; \bnfpn{F} \; \bnfpn{T'}} \\
		\bnfalt*{\bnfes} \\
		\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}\end{minipage}}
	\end{tiny}
\end{frame}

\subsubsection{Error recovery in predictive parsing}
\subsubsectiontableofcontentslide

\begin{frame}[background=6]{Panic Mode}
	\begin{itemize}
	\item \emph{Panic-mode error recovery is based on the idea of skipping over symbols on the input until a token in a selected set of synchronizing tokens appears}
	\vspace{.5cm}
	\item Its effectiveness depends on the choice of synchronizing set
	\item The sets should be chosen so that the parser recovers quickly from errors that are likely to occur in practice
	\vspace{.5cm}
	\item Some heuristics are explained in the following slides
	\end{itemize}
\end{frame}

\begin{frame}{Heuristics for Panic Mode}
	\begin{columns}
		\begin{column}{.5\linewidth}
			\simplebox{As a starting point, place all symbols in FOLLOW($A$) into the synchronizing set for nonterminal $A$. If we skip tokens until an element of FOLLOW($A$) is seen and pop $A$ from the stack, it is likely that parsing can continue}
			\vspace{.25cm}
			\simplebox{If we add symbols in FIRST($A$) to the synchronizing set for nonterminal $A$, then it may be possible to resume parsing according to $A$ if a symbol in FIRST($A$) appears in the input}
		\end{column}
		\begin{column}{.5\linewidth}
			\simplebox{It is not enough to use FOLLOW($A$) as the synchronizing set for $A$. We can add to the set of a lower-level construct the symbols that begin higher-level constructs. For example, we might add keywords that begin statements to the synchronizing sets for the nonterminals generating expressions}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Heuristics for Panic Mode \insertcontinuationtext}
	\begin{columns}
		\begin{column}{.5\linewidth}
			\simplebox{If a terminal on top of the stack cannot be matched, a simple idea is to pop the terminal, issue a message saying that the terminal was inserted, and continue parsing. In effect, this approach takes the synchronizing set of a token to consist of all other tokens}
		\end{column}
		\begin{column}{.5\linewidth}
			\simplebox{If a nonterminal can generate the empty string, then the production deriving $\bnfes$ can be used as a default. Doing so may postpone some error detection, but cannot cause an error to be missed. This approach reduces the number of nonterminals that have to be considered during error recovery}
		\end{column}
	\end{columns}	
\end{frame}

\subsection{Bottom-up parsing}
\subsectiontableofcontentslide

\sidecite{Knuth.1965, Korenjak.1969}
\begin{frame}{Bottom-Up Parsing}
	\begin{definitionblock}{Bottom-Up Parsing}
		Constructing a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root
	\end{definitionblock}
	\vspace{.5cm}
	\begin{block}{General Principle}
		Bottom-up parsing is the process of ``reducing'' a string $w$ to the start symbol of the grammar
	\end{block}
	\alertbox*{
		By definition, reduction is the reverse of derivation \\
		The goal of the bottom-up parsing is therefore to construct a derivation in reverse
	}
\end{frame}

\sidecite{Knuth.1965, Korenjak.1969}
\begin{frame}{Type of Bottom-Up Parsing}
	\centering\fancybox{Shift-reduce Parsing}{General form. Attached to the $LR(k)$ grammar class}{ascent-icon}{}
	\hspace{2cm}
	\begin{minipage}[b]{.4\linewidth}
			\begin{itemize}
			\item {\Large\Emph{L}}eft-to-right input scanning
			\item {\Large\Emph{R}}ightmost derivation
			\item {\Large\Emph{k}} input symbol is used for lookahead to make parsing action decisions
		\end{itemize}
	\end{minipage} \\[.25cm]
	\alertbox*{
		$LR(k)$ parser is too difficult to be written by hand \\
		We prefer to use automatic parser generators
	}
\end{frame}

\subsubsection{Reductions}
\subsubsectiontableofcontentslide

\begin{frame}{Principle of Reduction}
	\begin{definitionblock}{General Reduction Algorithm}
		At each reduction step:
		\begin{itemize}
			\item Select a specific substring matching the body of a production
			\item Replace the selected substring by the nonterminal at the head of the production
		\end{itemize}
	\end{definitionblock}
	\vspace{.5cm}
	\begin{block}{Key Decisions}
		\begin{itemize}
			\item when to reduce
			\item what production to apply
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[t]{Example of Reductions}
	\vspace{-.5cm}
	\begin{small}
	\begin{itemize}
	\item Let the grammar:
		\raisebox{-.65\height}{
		\begin{scriptsize}\begin{minipage}{.25\linewidth}\begin{bnf}
			\bnfprod*{E}{\bnfpn{T} \; \bnfpn{E'}} \\
			\bnfprod*{E'}{\bnfts{+} \; \bnfpn{T} \; \bnfpn{E'}} \\
			\bnfalt*{\bnfes} \\
			\bnfprod*{T}{\bnfpn{F} \; \bnfpn{T'}} \\			
			\bnfprod*{T'}{\bnfts{*} \; \bnfpn{F} \; \bnfpn{T'}} \\
			\bnfalt*{\bnfes} \\
			\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
			\bnfalt*{\bnfts{id}}
		\end{bnf}\end{minipage}\end{scriptsize}
	}
	\item A possible sequence of reductions is: \\
		{$\bnfts{id}~\bnfts{*}~\bnfts{id} \reduce~F~\bnfts*~\bnfts{id} \reduce~F~\bnfts{*}~F \reduce~F~\bnfts{*}~F~\bnfes \reduce~F~\bnfts{*}~F~T' \reduce~F~T'$} \\
		\mbox{\only<2>{$\reduce~T	\reduce~T~\bnfes \reduce~T~E' \reduce~E$}}
	\end{itemize}
	\end{small}
	\vfill
	\begin{center}
	\includeanimatedfigure[height=.25\paperheight]{reduction_example}
	\end{center}
\end{frame}

\begin{frame}[background=8]{How to Find the Best Reductions?}
	\alertbox{What is the best sequence of reductions to build the parse tree?}
	\vspace{4em}
	\begin{itemize}
	\item One method is to use the \emph{shift-reduce parsing} method
	\vspace{2em}
	\item Shift-reduce method is based on the \emph{handle pruning}
	\end{itemize}
\end{frame}

\subsubsection{Handle pruning}
\subsubsectiontableofcontentslide

\begin{frame}[t]{Definition of a Handle}
	\begin{definitionblock}{Handle}
		Substring matching the body of a production, and whose reduction represents one step along the reverse of a right-most derivation \\[.25cm]
		If $S \seqderivrm \alpha\;A\;\omega \derivrm \alpha\;\beta\;\omega$ then production $\bnfpn{A} \bnfopo \beta$ in the position following $\alpha$ is a handle of $\alpha\beta\omega$
	\end{definitionblock}
		\begin{center}
			\includegraphicswtex[width=.3\linewidth]{handle_definition}
		\end{center}
	\smaller
	\begin{itemize}
	\item A handle of a right-sentential form $\gamma$ is a production $\bnfpn{A} \bnfopo \beta$ and a position of $\gamma$ where the string $\beta$ may be found, such that replacing $\beta$ at that position by $A$ produces the previous right-sentential form in a rightmost derivation of $\gamma$
	\item \emph{Note that $\omega$ must contain only terminal symbols}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Algorithm of Handle Pruning}
	\begin{scriptsize}
	\begin{myalgorithm}
		\SetKwInOut{Assumption}{Assumption}
		\Inputs{A string of terminals $\omega$. A grammar $G = \langle N, \Sigma, P, S\rangle$}
		\Output{A sequence of reductions of $\omega$, or an error if no sequence was found}
		\Assumption{$w = \gamma_n$, where $\gamma_n$ is the n\textup{th} right-sentential form of some, \Emph{yet unknown}, rightmost derivation: $S = \gamma_0 \derivrm \gamma_1 \derivrm \gamma_2 \seqderivrm \gamma_{n-1} \derivrm \gamma_n = \omega$}
		\Begin{
			$d \affect []$; $f \affect \omega$\;
			\While{$f \neq S$}{
				\uIf{$\exists h \in f | f = \alpha\;h\;\beta\;;\,\beta$ contains only terminals}{
					\uIf{$\exists p \in G | p = (\bnfpn{A} \bnfopo h)$}{
						$f \affect \alpha\;p\;\beta$\;
						$d \affect [(\alpha\;h\;\beta)].d$\;
					}
					\Else{
						\throw{"Cannot find a production for reduction"}
					}
				}
				\Else{
					\throw{"Cannot find a handle"}
				}
			}
			\Return $d$\;
		}
	\end{myalgorithm}
	\end{scriptsize}
\end{frame}

\subsubsection{Shift-reduce parsing}
\subsubsectiontableofcontentslide

\begin{frame}[background=6]{Shift-Reduce Parsing}
	\begin{itemize}
		\item Shift-reduce parsing is a form of bottom-up parsing in which a \emph{stack holds grammar symbols} and an \emph{input buffer holds the rest of the string} to be parsed
		\vspace{1cm}
		\item The handle always appears at the top of the stack just before it is identified as the handle
	\end{itemize}
\end{frame}

\begin{frame}{Shift-Reduce Operations}
	\centering
	\fancybox{Shift}{Shift the next input symbol onto the top of the stack}{shift-icon}{01}
	\hspace{1cm}
	\fancybox{Reduce}{Get string from the stack, and detect the production to replace it}{reduce-icon}{02}
\end{frame}

\begin{frame}{Shift-Reduce Operations \insertcontinuationtext}
	\centering
	\fancybox{Accept}{Successful completion of parsing}{valid-icon}{03}
	\hspace{1cm}
	\fancybox{Error}{Syntax error and call an error recovery routine}{error-icon}{04}
\end{frame}

\begin{frame}{Example of Shift-Reduce Parsing}
	\begin{footnotesize}
	\begin{minipage}{.33\linewidth}\begin{bnf}
		\bnfprod*{E}{\bnfpn{T} \; \bnfpn{T'}} \\
		\bnfprod*{E'}{\bnfts{+} \; \bnfpn{T} \; \bnfpn{T'}} \\
		\bnfalt*{\bnfes} \\
		\bnfprod*{T}{\bnfpn{F} \; \bnfpn{T'}} \\
		\bnfprod*{T'}{\bnfts{*} \; \bnfpn{F} \; \bnfpn{T'}} \\
		\bnfalt*{\bnfes} \\
		\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
		\bnfalt*{\bnfts{id}}
	\end{bnf}\end{minipage}
	\end{footnotesize}
	\hfill
	\raisebox{-.5\height}{\includeanimatedfigure[width=.65\linewidth]{shift_reduce_parsing_example}}
\end{frame}

\subsubsection{Conflicts during shift-reduce parsing}
\subsubsectiontableofcontentslide

\begin{frame}{Conflicts During Shift-Reduce Parsing}
	\alertbox{There are context-free grammars for which shift-reduce parsing cannot be used}
	\vspace{.25cm}
	\alertbox*{Every shift-reduce parser can reach a configuration in which the parser, knowing the entire stack and also the next $k$ input symbols}
	\begin{center}
		\titlebox*[.4\linewidth]{Shift/reduce Conflict}{
			Cannot decide whether to shift or to reduce
		}
		\hspace{.5cm}
		\titlebox*[.4\linewidth]{Reduce/reduce Conflict}{
			Cannot decide which of several reductions to make
		}
	\end{center}
	\vspace{.25cm}
	\alertbox*{Grammars causing these conflicts are not in the $LR(k)$ class}
\end{frame}

\begin{frame}[background=9]{Example of Shift-Reduce Conflict}	
	\begin{itemize}
	\item Consider the grammar:
		\begin{footnotesize}\begin{bnf}
			\bnfprod*{statement}{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{statement}} \\
			\bnfalt*{\bnfts{if} \; \bnfpn{expression} \; \bnfts{then} \; \bnfpn{statement} \; \bnfts{else} \; \bnfpn{statement}} \\
			\bnfalt*{\bnfts{id}}
		\end{bnf}\end{footnotesize}
	\item Consider the stack : $\bnfts{eof} \dots \bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement}$
	\item Consider the input : $\bnfts{else} \dots \bnfts{eof}$
	\vfill
	\item We cannot tell whether $\bnfts{if} \bnfpn{expression} \bnfts{then} \bnfpn{statement}$ is the handle, no matter what appears below it on the stack. There is a shift/reduce conflict
	\item Depending on what follows the $\bnfts{else}$ on the input, it might be correct to reduce if-then to $\bnfpn{statement}$, or it might be correct to shift $\bnfts{else}$ and then to look for another $\bnfpn{statement}$ to complete the if-then-else
	\end{itemize}
\end{frame}

\begin{frame}[t]{Example of Reduce-Reduce Conflict}	
	\begin{itemize}
	\item Consider the grammar with array indexes between parenthesis:
		\begin{footnotesize}\begin{bnf}
			\bnfprod*{statement}{\bnfts{id} \bnfts{(} \bnfpn{parameters} \bnfts{)}} \\
			\bnfalt*{\bnfts{id} \bnfts{:=} \bnfpn{expression}} \\
			\bnfprod*{parameters}{\bnfpn{parameters} \bnfts{,} \bnfts{id}} \\
			\bnfalt*{\bnfts{id}} \\
			\bnfprod*{expression}{\bnfts{id} \bnfts{(} \bnfpn{expressions} \bnfts{)}} \\
			\bnfalt*{\bnfts{number}} \\
			\bnfprod*{expressions}{\bnfpn{expressions} \bnfts{,} \bnfpn{expression}} \\
			\bnfalt*{\bnfpn{expression}}
		\end{bnf}\end{footnotesize}
	\vspace{-.75cm}
	\item Consider the stack : $\bnfts{eof} \dots \bnfts{id} \bnfts{(} \bnfts{id} \bnfts{)}$
	\item Consider the input : $\bnfts{,} \bnfts{id} \bnfts{)} \dots \bnfts{eof}$
	\item It is evident that the $\bnfts{id}$ on top of the stack should be reduced, but by which production?
		\begin{enumerate}
		\item $\bnfpn{parameters} \bnfopo \bnfts{id}$ if $p$ is a procedure
		\item $\bnfpn{expressions} \bnfopo \bnfts{id}$ if $p$ is an array
		\end{enumerate}
	\end{itemize}
\end{frame}

\subsection{$LR(k)$ parsing}
\subsectiontableofcontentslide*

\subsubsection{Principles}

\sidecite{DeRemer.1971}
\begin{frame}{Simple $LR(k)$ Parsing}
	\alertbox*{This section introduces a simple $LR(k)$ (or $SLR(k)$) parsing based on the concepts previously presented}
	\vspace{.5cm}
	\begin{definitionblock}{$LR(k)$ Grammar}
		Class of grammars that have the following properties:
		\begin{itemize}
			\item {\Large\Emph{L}}eft-to-right input scanning
			\item {\Large\Emph{R}}ightmost derivation
			\item {\Large\Emph{k}} input symbol is used for lookahead to make parsing action decisions
		\end{itemize}
		$LR(k)$ parser is table-driven, as the nonrecursive $LL(k)$ parser
	\end{definitionblock}
	\vspace{.5cm}
	\alertbox{In this chapter, only cases $k=0$ and $k=1$ are considered}
\end{frame}

\begin{frame}{Why $LR(k)$ Parsers?}
	\begin{enumerate}
	\item Can be constructed to \emph{recognize all programming language} constructs \\
		Non-$LR$ context-free grammars exist, but not used for typical programming-language constructs
	\item $LR$-parsing method is the \emph{most general nonbacktracking shift-reduce parsing method}
	\item $LR$ parser can \emph{detect a syntactic error early}
	\item For a grammar to be $LR(k)$, we must be able to recognize the occurrence of the right side of a production in a right-sentential form, with $k$ input symbols of lookahead \\
		This requirement is far less stringent than that for $LL(k)$ grammars where we must be able to recognize the use of a production seeing only the first $k$ symbols of what its right side derives \\
		Thus, it should not be surprising that \emph{$LR$ grammars can describe more languages than $LL$ grammars}
	\end{enumerate}
\end{frame}

\subsubsection{$LR(0)$ automaton}
\subsubsectiontableofcontentslide

\begin{frame}[background=6]{Why $LR(0)$ Automaton?}
	\begin{itemize}
	\item \Emph{$LR(0)$ automaton helps with shift-reduce decisions}
	\vfill
	\item Suppose that the string $\gamma$ of symbols takes the $LR(0)$ automaton from the start state $I_0$ to some state $I_j$
	\vfill
	\item Then, shift on the next input symbol $\bnfts{a}$ if state $I_j$ has a transition on $\bnfts{a}$
	\vfill
	\item Otherwise, we choose to reduce \\
		\emph{Items} in state $I_j$ indicate which production to use
	\end{itemize}
\end{frame}

\begin{frame}[t]{Definition of Item}
	\simplebox{
		\begin{itemize}
		\item $LR(0)$ parser makes shift-reduce decisions by maintaining states to keep track of where we are in a parse
		\item States represent sets of ``items''
		\end{itemize}
	}
	\vspace{.25cm}
	\begin{definitionblock}{Item}
		$LR(0)$ item (or item) of a grammar $G$ is a production of $G$ with a \emph{dot} (noted $\bnfindex$) at some position of the body
	\end{definitionblock}
	\begin{itemize}
	\item Production $\bnfpn{A} \bnfopo XYZ$ generates the four items:
		\begin{footnotesize}\begin{minipage}[t]{.35\linewidth}\vspace{-.6cm}\begin{bnf}
			\bnfprod*{A}{\bnfindex \; X \; Y \; Z} \\
			\bnfalt*{X \; \bnfindex \; Y \; Z} \\
			\bnfalt*{X \; Y \; \bnfindex \; Z} \\
			\bnfalt*{X \; Y \; Z \; \bnfindex}
		\end{bnf}\end{minipage}\end{footnotesize}
	\item Production $\bnfpn{A} \bnfopo \bnfes$ generates only one item: $\bnfpn{A} \bnfpo \bnfindex$
	\end{itemize}
\end{frame}

\begin{frame}{{Informal Definition} of Item}
	\alertbox*{Intuitively, an item indicates how much of a production we have seen at a given point in the parsing process}
	\vspace{1cm}
	\begin{examples}
		\begin{enumerate}
			\item $\bnfpn{A} \bnfpo \bnfindex \; X \; Y \; Z$ \\
				we hope to see a string derivable from $X \; Y \; Z$ next on the input
			\item $\bnfpn{A} \bnfpo X \; \bnfindex \; Y \; Z$ \\
				we have just seen on the input a string derivable from $X$ and that we hope next to see a string derivable from $Y \; Z$
			\item $\bnfpn{A} \bnfpo X \; Y \; Z \; \bnfindex$ \\
				we have seen the body $X \; Y \; Z$ and that it may be time to reduce $X \; Y \; Z$ to $A$
		\end{enumerate}
	\end{examples}
\end{frame}

\begin{frame}{{Kernel Item} and Nonkernel Item}
	\begin{definitionblock}{Kernel Item}
		Initial item, $\bnfpn{S'} \bnfopo \bnfpn{S}$, and all items whose dots are not at the left end
	\end{definitionblock}
	\vspace{1cm}
	\begin{definitionblock}{Nonkernel Item}
		All items with their dots at the left end, except for $\bnfpn{S'} \bnfopo \bnfpn{S}$
	\end{definitionblock}
\end{frame}

\begin{frame}{Set of Items}
	\begin{definitionblock}{Set of Items in DFA}
		Each state of the $LR(0)$ automaton represents a set of items in the canonical $LR(0)$ collection
	\end{definitionblock}
	\vspace{.5cm}
	\begin{definitionblock}{Canonical $LR(0)$ Collection}
		Collection of sets of $LR(0)$ items, used as the basis for constructing a deterministic finite automaton
	\end{definitionblock}
	\vspace{.5cm}
	\hiconbox{
		To construct the canonical $LR(0)$ collection, we define:
		\begin{enumerate}
			\item an \emph{augmented grammar}
			\item the functions \emph{CLOSURE} and \emph{GOTO}
		\end{enumerate}}{method-icon}
\end{frame}

\begin{frame}{Augmented Grammar}
	\begin{definitionblock}{Augmented Grammar}
		Let $G = \langle N, \Sigma, P, S\rangle$ \\
		Augmented grammar $G'$ of $G$ is defined by $\langle N, \Sigma, P \cup \left(\bnfpn{S'} \bnfopo \bnfpn{S}\right), S'\rangle$
	\end{definitionblock}
	\vspace{.25cm}
	\begin{itemize}
	\item The purpose of the new starting production is to indicate to the parser when it should stop parsing and announce acceptance of the input
	\vspace{.25cm}
	\item Acceptance occurs when and only when the parser is about to reduce by $\bnfpn{S'} \bnfopo \bnfpn{S}$
	\end{itemize}
\end{frame}

\begin{frame}{CLOSURE: Closure of Item Sets}
	\begin{myalgorithm}
		\Input{Set $I$ of items for grammar $G = \langle N, \Sigma, P, S\rangle$}
		\Output{CLOSURE($I$)}
		\BlankLine
		\Begin{
			CLOSURE($I$) \affect $I$ \;
			\While{CLOSURE($I$) has been changed}{
				\ForEach{$\left(\bnfpn{A} \bnfopo \alpha \; \bnfindex \; B \; \beta\right) \in$ CLOSURE($I$)}{
					\If{$\left(\bnfpn{B} \bnfopo \gamma\right) \in P$}{
						CLOSURE($I$) \affect CLOSURE($I$) $\cup \left(\bnfpn{B} \bnfopo \bnfindex \; \gamma\right)$ \;
					}
				}
			}			 
		}
	\end{myalgorithm}
\end{frame}

\begin{frame}[t]{Example of CLOSURE}
	\begin{center}\scriptsize
		\vspace{-.5cm}\begin{minipage}{.4\linewidth}\begin{bnf}
			\bnfprod*{E'}{\bnfpn{E}} \\
			\bnfprod*{E}{\bnfpn{E} \; \bnfts{+} \; \bnfpn{T}} \\
			\bnfalt*{\bnfpn{T}} \\
			\bnfprod*{T}{\bnfpn{T} \; \bnfts{*} \; \bnfpn{F}} \\
			\bnfalt*{\bnfpn{F}} \\
			\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
			\bnfalt*{\bnfts{id}}
		\end{bnf}\end{minipage}
	\end{center}
	\begin{scriptsize}
	\only<1->{
		\begin{tabular}[t]{|c|}
			\hline
			$I_0$ \\
			\hline
			$\bnfpn{E'} \bnfopo \bnfindex \; \bnfpn{E}$ \\
			\hline
		\end{tabular}
	}
	\only<2->{
		\raisebox{-\height}{\pgfuseimage{rightarrow-in-code}}
		\begin{tabular}[t]{|c|}
			\hline
			$I_0$ \\
			\hline
			$\bnfpn{E'} \bnfopo \bnfindex \; \bnfpn{E}$ \\
			$\bnfpn{E} \bnfopo \bnfindex \; \bnfpn{E} \bnfts{+} \bnfpn{T}$ \\
			$\bnfpn{E} \bnfopo \bnfindex \; \bnfpn{T}$ \\
			\hline
		\end{tabular}
	}
	\only<3>{
		\raisebox{-\height}{\pgfuseimage{rightarrow-in-code}}
		\begin{tabular}[t]{|c|}
			\hline
			$I_0$ \\
			\hline
			$\bnfpn{E'} \bnfopo \bnfindex \; \bnfpn{E}$ \\
			$\bnfpn{E} \bnfopo \bnfindex \; \bnfpn{E} \; \bnfts{+} \bnfpn{T}$ \\
			$\bnfpn{E} \bnfopo \bnfindex \; \bnfpn{T}$ \\
			$\bnfpn{T} \bnfopo \bnfindex \; \bnfpn{T} \; \bnfts{*} \; \bnfpn{F}$ \\
			$\bnfpn{T} \bnfopo \bnfindex \; \bnfpn{F}$ \\
			\hline
		\end{tabular}
		\raisebox{-1em}{\Huge\dots}
	} \\[1em]
	\end{scriptsize}
	\small
	\only<1>{$I = \{ (E' \bnfopo \bnfindex \; E) \}$ and CLOSURE($I$) $= I_0$ \\}
	\only<2>{Consider $E$-productions because $E$ is on the right of the dot. Add $\bnfpn{E} \bnfopo \bnfindex \; \bnfpn{E} \bnfts{+} \bnfpn{T}$ and $\bnfpn{E} \bnfopo \bnfindex \; \bnfpn{T}$ to $I_0$ \\}
	\only<3>{Consider $E$-productions and $T$-productions because they are both on the right of the dot. Items for $E$-productions are already inside $I_0$, but not items for $T$-productions. \\}
\end{frame}

\begin{frame}{GOTO}
	\begin{definitionblock}{GOTO Function}
	Closure of the set of all items $\bnfpn{A} \bnfopo \alpha \; \bnfpn{X} \; \bnfindex \; \beta$ \\
	Such that $\left(\bnfpn{A} \bnfopo \alpha \; \bnfindex \; \bnfpn{X} \; \beta\right) \in I$ \\
	Where $I$ is a set of items, and $X$ is a grammar symbol
	\end{definitionblock}
	\vspace{1cm}
	\begin{itemize}
	\item Intuitively, the GOTO function is used to define the transitions in the $LR(0)$ automaton for a grammar
	\item States of the automaton corresponds to sets of items, and GOTO($I,X$) specifies the transition from the state $I$ under input $X$
	\end{itemize}
\end{frame}

\begin{frame}{Example of GOTO}
	\begin{center}\scriptsize
		\begin{minipage}{.4\linewidth}\begin{bnf}
			\bnfprod*{E'}{\bnfpn{E}} \\
			\bnfprod*{E}{\bnfpn{E} \; \bnfts{+} \; \bnfpn{T}} \\
			\bnfalt*{\bnfpn{T}} \\
			\bnfprod*{T}{\bnfpn{T} \; \bnfts{*} \; \bnfpn{F}} \\
			\bnfalt*{\bnfpn{F}} \\
			\bnfprod*{F}{\bnfts{(} \; \bnfpn{E} \; \bnfts{)}} \\
			\bnfalt*{\bnfts{id}}
		\end{bnf}\end{minipage}
	\end{center}
	\begin{itemize}
	\item If $I$ is the set of two items $\left\{ \; (\bnfpn{E'} \bnfopo \bnfpn{E} \; \bnfindex), \; (\bnfpn{E} \bnfopo \bnfpn{E} \; \bnfindex \; \bnfts{+} \; \bnfpn{T}) \; \right\}$
	\item Then, GOTO($I,\bnfts{+}$) = CLOSURE( $\{ \; (\bnfpn{E} \bnfopo \bnfpn{E} \; \bnfts{+} \; \bnfindex \; \bnfpn{T}) \; \}$ is \\
		\begin{center}\begin{scriptsize}
		\begin{tabular}[t]{|c|}
			\hline
			$\bnfpn{E} \bnfopo \bnfpn{E} \; \bnfts{+} \; \bnfindex \; \bnfpn{T}$ \\
			$\bnfpn{T} \bnfopo \bnfindex \; \bnfpn{T} \; \bnfts{*} \; \bnfpn{F}$ \\
			$\bnfpn{T} \bnfopo \bnfindex \; \bnfpn{F}$ \\
			$\bnfpn{F} \bnfopo \bnfindex \; \bnfts{(} \; \bnfpn{E} \; \bnfts{)}$ \\
			$\bnfpn{T} \bnfopo \bnfindex \; \bnfpn{id}$ \\
			\hline
		\end{tabular}
		\end{scriptsize}\end{center}
	\end{itemize}
\end{frame}

\begin{frame}[background=9]{$LR(0)$ Automaton}
	\begin{itemize}
	\item Simple $LR$ (or $SLR$) parsing constructs $LR(0)$ automaton from the grammar
	\item States of this automaton are the sets of items from the \emph{canonical $LR(0)$ collection}, and the transitions are given by the GOTO function
	\vspace{1cm}
	\item In the following slide, there is an example of $LR(0)$ automaton
		\begin{itemize}
		\item Kernel items are in the \colorbox{DA53yellow}{\textcolor{black}{light-yellow}} part of the box
		\item Nonkernel items are in the \colorbox{DA53darkyellow}{\textcolor{black}{dark-yellow}} part of the box
		\item Egde represents the transitions given by the function GOTO, where the label is the token name
		\end{itemize}
	\end{itemize}
\end{frame}

\figureslide{Example of $LR(0)$ Automaton}{lr0_automaton}

\begin{frame}[t]{Algorithm to Compute the Canonical $LR(0)$ Collection}
	\begin{myalgorithm}
	\Input{Augmented grammar $G'$}
	\Output{Canonical $LR(0)$ Collection, namely $C$}
	\BlankLine
	\Begin{
		$C$ \affect CLOSURE( $\{ \; (\bnfpn{S'} \bnfopo \bnfindex \; \bnfpn{S}) \; \}$ )\;
		\Repeat{no new sets of items are added to $C$ on a round}{
			\ForEach{set of items $I \in C$}{
				\ForEach{grammar symbol in $X$}{
					\If{GOTO($I,X$) is not empty and not in $C$}{
						$C$ \affect $C$ $\cup$ GOTO($I, X$)\;
					}
				}
			}
		}
		\Return $C$\;
	}
	\end{myalgorithm}
\end{frame}

\subsubsection{$LR$ parsing algorithm}
\subsubsectiontableofcontentslide

\begin{frame}[t]{Algorithm for $LR$-Parsing \insertcontinuationwith{1}}
	\begin{itemize}
	\item $LR$ parser consists of an \emph{input}, an \emph{output}, a \emph{stack}, a \emph{driver program}, and a \emph{parsing table} that has two parts: ACTION and GOTO
	\item \Emph{Only the parsing table change from one parser to another}
	\vspace{.25cm}
	\item Parsing program reads characters from an input buffer one at a time
	\item It \Emph{shifts a state; not a symbol}. \emph{This is a major difference between a $LR$ parser and a shift-reduce parser}
	\end{itemize}
	\putat*(80,-160){\pgfuseimage{lr-parser}}
\end{frame}

\begin{frame}[t]{Algorithm for $LR$-Parsing \insertcontinuationwith{2}}
	\begin{small}
	\begin{itemize}
	\item Stack holds a sequence of states, $s_0\;s_1 \dots s_m$, where $s_m$ is on top \footnote{In $SLR$ method, the stack holds the states from the $LR(0)$ automaton; the canonical $LR$ and $LALR$ methods are similar} 
	\item All the transition that are entering in a state are labeled with the same symbol \\
	State may be associated to one symbol and only one, except for the start state
	\end{itemize}
	\end{small}
	\putat*(80,-160){\pgfuseimage{lr-parser}}
\end{frame}

\begin{frame}[background=9]{ACTION in the $LR$-Parsing Table}
	\begin{itemize}
	\item This function takes as arguments a state $s_i$ and a terminal $\bnfts{a}$ (or \bnfts{eof}).
	\vfill
	\item The value of ACTION$[s_i,a]$ can have one of the four forms:
		\begin{enumerate}
		\item \Emph{Shift} $j$, where $s_j$ is a state \\
			Action taken by the parser effectively shifts input a to the stack, but uses state $s_j$ to represent $a$
		\item \Emph{Reduce} $\bnfpn{A} \bnfopo \beta$ \\
			Action of the parser effectively reduces $\beta$ on the top of the stack to head $A$
		\item \Emph{Accept} \\
			Parser accepts the input and terminates
		\item \Emph{Error} \\
			Parser discovers an error and takes some corrective action
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}[background=6]{GOTO in the $LR$-Parsing Table}
	\begin{itemize}
	\item The GOTO function, defined on sets of items, is extended to states.
	\vspace{1cm}
	\item If GOTO[$I_i,A$] $= I_j$, then GOTO also maps a state $I_i$ and a nonterminal $A$ to state $I_j$.
	\end{itemize}
\end{frame}

\begin{frame}[t]{Algorithm for $LR$-Parsing}
	\begin{scriptsize}
	\vspace{-.23cm}
	\begin{myalgorithm}
	\SetKwFunction{inputSymbol}{inputSymbol}
	\SetKwFunction{push}{push}
	\SetKwFunction{pop}{pop}
	\SetKwFunction{topOf}{topOf}
	\SetKwFunction{Shift}{Shift}
	\SetKwFunction{Reduce}{Reduce}
	\SetKwFunction{Accept}{Accept}
	\SetKwFunction{print}{print}
	\Input{An input string $w$ and an $LR$-parsing table with functions ACTION and GOTO for a grammar $G$}
	\Output{If $w$ is in $L(G)$, the reduction steps of a bottom-up parse for $w$; otherwise, an error indication}
	\BlankLine
	\Begin{
		$S$ \affect $[ s_0 ]$; $a$ \affect \inputSymbol() \;
		$stopParser$ \affect \code{false} \;
		\While{$\neg stopParser$}{
			$s$ \affect \topOf($S$) \;
			\uIf{ACTION[$s,a$] = \Shift($t$)}{
				\push($S, t$) \;
				$a$ \affect \inputSymbol() \;
			}
			\uIf{ACTION[$s,a$] = \Reduce($\bnfpn{A} \bnfopo \beta$)}{
				\pop($S, \beta$) \;
				$t$ \affect \topOf($S$) \;
				\push($S$, GOTO($t,A$))\;
				\print( $\bnfpn{A} \bnfopo \beta$) \;
			}
			\uIf{ACTION[$s,a$] = \Accept}{
				$stopParser$ \affect \code{true} \;
			}
			\Else{
				\throw{"No production found"}
			}
		}
	}
	\end{myalgorithm}
	\end{scriptsize}
\end{frame}

\subsubsection{Building $SLR$-parsing table}
\subsubsectiontableofcontentslide

\begin{frame}[background={10}]{Building the $SLR$-Parsing Table}
	\begin{itemize}
	\item $SLR$ method refers to the parsing table, the SLR table
	\vfill
	\item $SLR$ method begins with $LR(0)$ items and $LR(0)$ automata:
		\begin{enumerate}
		\item Given a grammar $G$, we augment $G$ to produce $G'$, with a new start symbol $S'$
		\item From $G'$, we construct $C$, the canonical collection of sets of items for $G'$ together with the GOTO function
		\item ACTION and GOTO entries in the parsing table are then constructed using the algorithm in the following slides
		\item This algorithm requires us to know FOLLOW(A) for each nonterminal $A$ of the grammar
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Algorithm for Building the $SLR$-Parsing Table}
	\tiny\larger
	\vspace{-.24cm}
	\begin{myalgorithm}
		\Input{Augmented grammar $G'$}
		\Output{$SLR$-parsing table functions ACTION and GOTO for $G'$}
		\Begin{
			$C$ \affect $\{ I_0, I_1, \dots, I_n \}$ \tcp*[f]{Sets of $LR(0)$ items for $G'$} \;
			\For{$i$ \affect $1$ \KwTo $n$} {
				\uIf{$\left(\bnfpn{A} \bnfopo \alpha \; \bnfindex \; \alpha \; \beta\right) \in I_i$ and GOTO($I_i, \bnfts{a}$)$=I_j$}{
					ACTION[$i,a$] \affect "Shift $j$"
				}
				\uElseIf{$\left(\bnfpn{A} \bnfopo \alpha \; \bnfindex \; \alpha\right) \in I_i$}{
					\ForEach{$\bnfts{a} \in$ FOLLOW($A$) and $A \neq S'$}{
						ACTION[$i,\bnfts{a}$] \affect "Reduce $\bnfpn{A} \bnfopo \alpha$"
					}
				}
				\uElseIf{$\left(\bnfpn{S'} \bnfopo \bnfpn{S}\right) \in I_i$}{
					ACTION[$i,\bnfts{a}$] \affect "Accept"
				}
				\Else{
					\throw{"G is not SLR(1)"}
				}
				\ForEach{nonterminal $A$}{
					\If{GOTO($I_i,A$) $= I_j$}{
						GOTO($s_i,A$) \affect $s_j$
					}
				}
			}
		$S_0$ \affect state that corresponds to item $\bnfpn{S'} \bnfopo \bnfpn{S} \; \bnfindex$ \;
		}
	\end{myalgorithm}
\end{frame}

\subsubsection{$LALR$ parsing}
\subsubsectiontableofcontentslide

\sidecite{DeRemer.1969}
\begin{frame}[background={10}]{$LALR$ Parsing}
	\begin{itemize}
	\item $LALR$ parsing (\emph{LookAhead $LR$ parsing}) if often used in practice (details in the reference books)
	\item Tables obtained by $LALR$ methods are significantly smaller than tables obtained by canonical $LR$ methods
	\item $LALR$ parsers offer many of the advantages of $SLR$ and canonical-$LR$ parsers
	\item They combine the states that have the same kernels (sets of items, ignoring the associated lookahead sets)
	\item Thus, the number of states is the same as that of the $SLR$ parser, but some parsing-action conflicts present in the $SLR$ parser may be removed in the $LALR$ parser
	\item $LALR$ parsers have become the method of choice in practice
	\end{itemize}
\end{frame}

\section[Parser Generators]{Generate a syntactic parser with Yacc or JavaCC}
\sectiontableofcontentslide

\subsection{Overview}

\sidecite{Johnson.1975}
\begin{frame}{Parser Generators}
	\begin{itemize}
	\item Parser generators such as Yacc and its more recent implementation Bison, are generally LALR parser generators
	\item They permit to facilitate the creation of the front-end of a compiler by generating the source code from a grammar and a lexical analyzer specification
	\vfill
	\item This section describes two families of parser generators:
		\begin{itemize}
		\item Yacc, or Bison, that generates C and C++ parsers
		\item JavaCC, that generates Java parsers
		\end{itemize}
	\end{itemize}
	\begin{center}
		\pgfuseimage{parser-tool}
	\end{center}
\end{frame}

\subsection{Yacc/Bison}
\subsectiontableofcontentslide*

\subsubsection{Using Yacc/Bison}

\figureslide{Process of Yacc}{yacc_process}

\begin{frame}[t,fragile]{Structure of a Yacc Program}
	\begin{itemize}
	\item A Yacc program has the following form:
		\begin{lstlisting}[language=C]
		Declarations
		%%
		Translation rules
		%%
		Auxiliary functions
		\end{lstlisting}
	\end{itemize}
	\begin{small}
	\only<1>{\begin{block}{Declarations}
		\begin{itemize}
		\item C ordinary declarations, between \code{\%\{} and \code{\%\}}
		\item Declarations of tokens with the command \code{\%token}
		\end{itemize}
	\end{block}}
	\only<2>{\begin{block}{Translation rules}
		Each rule consists of a grammar production and the associated action (note the final semicolon) \\
		\begin{tabular}{@{}lcl@{}}
		{\textless}head{\textgreater}&:&{\textless}body$_1${\textgreater} \{ {\textless}action$_1${\textgreater} \} \\
		&$|$&{\textless}body$_2${\textgreater} \{ {\textless}action$_2${\textgreater} \} \\
		&&\dots \\
		&$|$&{\textless}body$_n${\textgreater} \{ {\textless}action$_n${\textgreater} \} \\
		&;&
		\end{tabular}
	\end{block}}
	\only<3>{\begin{block}{Auxiliary functions}
		\begin{itemize}
		\item Auxiliary functions are the section where additional C routines should be put
		\item Note that you must provide the function \code{yylex()}, which is invoking the lexical analyzer (explained later)
		\end{itemize}
	\end{block}}
	\end{small}
\end{frame}

\begin{frame}[fragile]{Example of a Yacc Program}
	\vspace{-.24cm}
	\begin{lstlisting}[style=lststyle-c,basicstyle=\tiny]
	%{
	    #include <ctype.h>
	%}
	%token DIGIT
	%%
	line   :   expr '\n'        { printf("%d\n", $1); }
	         ;
	expr   :   expr '+' term    { $$ = $1 + $3; }
	         | term
	         ;
	term   :   term '*' factor  { $$ = $1 * $3; }
	         | factor
	         ;
	factor :   '(' expr ')'     { $$ = $2; }
	         | DIGIT
	         ;
	%%
	int yylex() {
	   int c;
	   c = getchar();
	   if (isdigit(c)) {
	      yylval = c - '0'; /* convert char to int */
	      return DIGIT;
	   }
	   return c;
	}
	\end{lstlisting}
\end{frame}

\subsubsection{Ambiguous grammar}
\subsubsectiontableofcontentslide

\begin{frame}{Special Yacc Operators}
	\begin{itemize}
	\item Yacc provides a set of declarations that may be used to remove grammar ambiguity.
	\vfill
	\item Associativity and Precedence:
		\begin{description}
		\item[Left associativity] \%left {\textless}op1{\textgreater} {\textless}op2{\textgreater}\dots
		\item[Right associativity] \%right {\textless}op3{\textgreater} {\textless}op4{\textgreater}\dots
		\item[No associativity] \%nonassoc {\textless}op5{\textgreater} {\textless}op6{\textgreater}\dots
		\item The tokens are given precedences in the order in which they appear in  the declaration part, lower first
		\item Tokens in the same declaration have the same precedence
		\end{description}
	\end{itemize}
\end{frame}

\subsubsection{Connecting to Lex}
\subsubsectiontableofcontentslide

\begin{frame}{{Use Lex} in Conjonction with Yacc}
	\begin{itemize}
	\item Lex was designed to produce lexical analyzers that could be used with Yacc
	\vfill
	\item Lex library provides a driver program named \ccode{yylex()}
	\vfill
	\item To use Lex in Yacc, you must remove any definition of \ccode{yylex() }in the Yacc specification; and replace this definition by:
		\begin{center}
		\ccode{\#include "lex.yy.c"}
		\end{center}
	\vfill
	\item All the tokens defined in the Yacc declarations are directly available in the Lex program
	\end{itemize}
\end{frame}

\subsubsection{Error recovery}
\subsubsectiontableofcontentslide

\begin{frame}{Error Production}
	\begin{itemize}
	\item In Yacc, error recovery uses a form of error productions
	\item First, you must decides what ``major'' nonterminals will have error recovery associated to them
	\item Typical choices are some subset of the nonterminals generating expressions, statements, blocks, and functions
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Error Recovery Functions}
	\begin{itemize}
	\item \ccode{yyerror()} reports an error
	\item \ccode{yyerrok()} resets the parser to its normal mode of operation
	\vfill
	\item \small Here, the error production causes the program to suspend normal parsing when a syntax error is found on an input line
	\item \small On encountering the error, the parser in the program starts popping symbols from its stack until it encounters a statethat as a shift action on the token error. Then the input is read until the new-line character is read. Then the parser reduces error '{\textbackslash}n' to lines, and emits the diagnotic message ``error message''
	\end{itemize}
	\begin{lstlisting}[style=lststyle-c]
	line   :   lines expr '\n'  { printf("%g\n", $2); }
	         | lines '\n'
	         | /* empty or epsilon */
	         | error '\n'       { yyerror("error message");
                                  yyerrok(); }
	\end{lstlisting}
\end{frame}

\subsection{JavaCC}
\subsectiontableofcontentslide*

\subsubsection{Using JavaCC}

\figureslide{Process of JavaCC}{javacc_process}

\begin{frame}[t,fragile]{Structure of a JavaCC Program}
	\begin{itemize}
	\item A JavaCC program has the following form:
		\begin{lstlisting}[style=lststyle-java]
		JavaCC options
		PARSER_BEGIN(<parserName>)
		Java compilation unit
		PARSER_END(<parserName>)
		Translation rules
		\end{lstlisting}
	\end{itemize}
	\begin{small}
	\only<1>{\begin{block}{Parser Definition}
		\begin{itemize}
		\item The name that follows ``PARSER\_BEGIN'' and ``PARSER\_END'' must be the same and this identifies the name of the generated parser
		\end{itemize}
	\end{block}}
	\only<2>{\begin{block}{Options}
		\begin{itemize}
		\item JavaCC options permits to control the behavior of the parser
		\end{itemize}
	\end{block}}
	\only<3>{\begin{block}{Java compilation unit}
		The Java compilation unit is a Java code that must contain at least the declaration of the class of the parser: \\
		\dots \\
		\code{public class} \ccode{{\textless}parser\_name{\textgreater} \{} \\
		\dots \\
		\ccode{\}} \\
		\dots
	\end{block}}
	\only<4>{\begin{block}{Predefined functions in the \ccode{parser\_name} class}
		Two functions are automatically generated inside the parser class:
		\begin{itemize}
		\item \code{Token getNextToken()}: returns the next available token
		\item \code{Token getToken(int index)}: returns the ith token ahead
		\end{itemize}
	\end{block}}
	\only<5>{\begin{block}{Translation rules}
		\begin{itemize}
		\item Java code production (see error recovery for an example)
		\item Regular expression definitions for tokens
		\item Grammar (BNF-like) productions
		\end{itemize}
	\end{block}}
	\end{small}
\end{frame}

\begin{frame}{Define Regular Expressions for Tokens}
	\begin{definition}
		\ccode{[{\textless}state\_list{\textgreater}] {\textless}kind{\textgreater} [\bnfts{IGNORE\_CASE}] \bnfts{:}} \\
		\ccode{\{ {\textless}regexpr{\textgreater} $|$ {\textless}regexpr{\textgreater} $|$ \dots \}}
	\end{definition}
	\begin{itemize}
	\item \ccode{{\textless}state\_list{\textgreater}} specifies the lexer states in which the rule is enabled (default is \ccode{DEFAULT})
	\item \bnfts{IGNORE\_CASE} specifies, by its presence, that if the regular expression is case sensitive or case insensitive
	\item The regular definitions are defined and used as follows, respectively (The \code{"\#"} before the id indicates that this definition exists solely for the purpose of defining other tokens): \\
		\bnfts{\textless} [\bnfts{\#}]id \bnfts{:} regexpr \bnfts{\textgreater} \\
		\bnfts{\textless}id\bnfts{\textgreater}
	\end{itemize}
\end{frame}

\begin{frame}{Types of Regular Expression Definitions \ccode{{\textless}kind{\textgreater}}}
	\begin{enumerate}
	\item[TOKEN] describes tokens in the grammar. The token manager creates a Token object for each match of such a regular expression and returns it to the parser
	\vfill
	\item[SPECIAL\_TOKEN] like tokens, except that they do not have significance during parsing, ie. the BNF productions ignore them
	\vfill
	\item[SKIP] simply skipped (ignored) by the token manager
	\vfill
	\item[MORE] Sometimes it is useful to gradually build up a token to be passed on to the parser. Matches to this kind of regular expression are stored in a buffer until the next \bnfts{TOKEN} or \bnfts{SPECIAL\_TOKEN} match
	\end{enumerate}
\end{frame}

\begin{frame}{Attributes of the Predefined \ccode{Token} Class}
	\begin{itemize}
	\item \code{int kind} \\
		This is the index for this kind of token in the internal representation scheme of JavaCC. It may be replaced by a constant
	\vfill
	\item \code{int beginLine, beginColumn, endLine, endColumn} \\
		The beginning and ending positions of the token as it appeared in the input stream
	\vfill
	\item \code{String image} \\
		The lexeme of the token as it appeared in the input stream
	\vfill
	\item \code{Token next} \\
		A reference to the next regular (non-special) token from the input stream
	\end{itemize}
\end{frame}

\begin{frame}{Methods of the Predefined \ccode{Token} Class}
	\begin{itemize}
	\item \code{Object getValue()} \\
		An optional attribute value of the \ccode{Token}. Tokens which are not used as syntactic sugar will often contain meaningful values that will be used later on by the compiler or interpreter. This attribute value is often different from the image. Any subclass of Token that actually wants to return a non-null value can override this method as appropriate
	\vfill
	\item \code{static final Token newToken(int ofKind)} \\
		\code{static final Token newToken(int ofKind, String image)} \\
		Returns a new token object as its default behavior
	\end{itemize}
\end{frame}

\begin{frame}[t]{Grammar Productions}
	\begin{definition}\small
		\ccode{{\textless}access\_modifier{\textgreater} {\textless}return\_type{\textgreater} {\textless}identifier{\textgreater} \bnfts( {\textless}parameters{\textgreater} \bnfts) \bnfts:} \\
		\ccode{{\textless}java\_block{\textgreater}} \\
		\ccode{\bnfts\{ {\textless}expansion\_choices{\textgreater} \bnfts\}}
	\end{definition}
	\begin{itemize}
	\item Name of the non-terminal $\Rightarrow$ name of the method
		\begin{itemize}
		\item parameters and return value depends on the goal of the compiler
		\end{itemize}
	\item Calls to non-terminals $\Rightarrow$ function calls
	\item Default access modifier: \code{public}
	\end{itemize}
\end{frame}

%\begin{frame}[t]{Heads of the Productions}
%	\begin{definition}\small
%		\ccode{{\textless}access\_modifier{\textgreater} {\textless}return\_type{\textgreater}} \\
%		\ccode{{\textless}identifier{\textgreater} \bnfts( {\textless}parameters{\textgreater} \bnfts) \bnfts:} \\
%		\ccode{{\textless}java\_block{\textgreater}} \\
%		\ccode{\bnfts\{ {\textless}expansion\_choices{\textgreater} \bnfts\}}
%	\end{definition}
%	\begin{itemize}
%	\item The name of the non-terminal is the name of the method, and the parameters and return value declared are the means to pass values up and down the parse tree
%	\item Non-terminals on the right hand sides of productions are written as method calls, so the passing of values up and down the tree are done using exactly the same paradigm as method call and return
%	\item The default access modifier for BNF productions is public
%	\end{itemize}
%\end{frame}

\begin{frame}[t]{Bodies of the Productions}
	\begin{definition}\small
		\ccode{{\textless}access\_modifier{\textgreater} {\textless}return\_type{\textgreater} {\textless}identifier{\textgreater} \bnfts( {\textless}parameters{\textgreater} \bnfts) \bnfts:} \\
		\ccode{{\textless}java\_block{\textgreater}} \\
		\ccode{\bnfts\{ {\textless}expansion\_choices{\textgreater} \bnfts\}}
	\end{definition}
	\begin{description}
	\item[Java block] arbitrary Java declarations and code put at the beginning of the method generated for the Java non-terminal
	\item[Expansion choices] a sequence of expansion units. Each nonterminal is written as a function call. Semantic actions are Java blocks inside this part of the BNF production
	\end{description}
\end{frame}

\begin{frame}[fragile]{Example of JavaCC File \insertcontinuationwith{1}}
	\begin{lstlisting}[style=lststyle-java]
	PARSER_BEGIN(CalculatorParser)
	    public class CalculatorParser {
	    }
	PARSER_END(CalculatorParser)

	SKIP: {
	       " "
	    |  "\t"
	    |  "\n"
	    |  "\r"
	}

	TOKEN :
	{
	     <DIGIT : [0-9]>
	}
	\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Example of JavaCC File \insertcontinuationwith{2}}
	\begin{columns}
		\begin{column}{.35\linewidth}\smaller
\begin{bnf}
	\bnfprod*{line}{\bnfpn{expr}} \\
	\bnfprod*{expr}{\bnfpn{expr} \bnfts{+} \bnfpn{term}} \\
	\bnfalt*{\bnfpn{term}} \\
	\bnfprod*{term}{\bnfpn{term} \bnfts{*} \bnfpn{factor}} \\
	\bnfalt*{\bnfpn{factor}} \\
	\bnfprod*{factor}{\bnfts{(} \bnfpn{expr} \bnfts{)}} \\
	\bnfalt*{\bnfts{digit}}
\end{bnf}
		\end{column}
		\begin{column}{.65\linewidth}
\begin{lstlisting}[style=lststyle-java]
private void line() :
{
  int e;
}
{ e = expr()       { System.out.println(e); }
}

private int expr() :
{
	int e, t;
}
{ e = expr() "+" t = term()   { return e+t; }
  | t = term()                { return v; }
}
\end{lstlisting}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[fragile]{Example of JavaCC File \insertcontinuationwith{3}}
	\begin{lstlisting}[style=lststyle-java]
	private int term() :
	{
	    int t, f;
	}
	{ t = term() "*" f = factor()   { return t*f; }
	  | f = factor()                { return f; }
	}

	private int factor() :
	{
	    int e, d;
	}
	{ "(" e = expr() ")"            { return e; }
	  | d = <DIGIT>                 { return Integer.parseInt(d.image); }
	}
	\end{lstlisting}
\end{frame}

\subsubsection{Error recovery}
\subsubsectiontableofcontentslide

\begin{frame}{Error Reporting with JavaCC}
	\begin{description}
	\item[Modify file \ccode{ParseException.java}] e.g. changing  \ccode{getMessage} method for customizing error reporting
	\item See Javadoc in \ccode{ParseException.java} and \ccode{TokenMgrError.java} for details
	\vfill
	\item [Override or call \ccode{generateParseException()}] (in the generated parser) \\
		it generates an object of type \ccode{ParseException}
	\end{description}
\end{frame}

\begin{frame}{Error Recovery with JavaCC}
	JavaCC offers two kinds of error recovery:
	\vfill
	\begin{enumerate}
	\item[Shallow recovery] recovers if none of the current choices have succeeded in being selected
	\vfill
	\item[Deep recovery] is when a choice is selected, but then an error happens sometime during the parsing of this choice
	\end{enumerate}
\end{frame}

\begin{frame}[t,fragile]{Shallow Error Recovery}
	When no token found, we want to skip until the next given symbol (semi-column) \\
	\begin{lstlisting}[style=lststyle-java]
	TOKEN : { <SEMICOLON: ";"> }
	private void stm() :
	{}
	{ ifStm()
	  | whileStm()
	}
	\end{lstlisting}
	\begin{center}\pgfuseimage{bottom-arrow}\end{center}
	\begin{lstlisting}[style=lststyle-java]
	TOKEN : { <SEMICOLON: ";"> }
	private void stm() :
	{}
	{ ifStm()
	  | whileStm()
	  | error_skipto(SEMICOLON)
	}
	\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Definition of the function \ccode{error\_skipto()}}
	\begin{itemize}
	\item \ccode{error\_skipto()} is a nonterminal that must be define prior to its first usage
	\item To do so, we must use the following code:
	\end{itemize}
	\begin{lstlisting}[style=lststyle-java]
	//JAVACODE
	void error_skipto(int kind) {
	    ParseException e = generateParseException()
	    System.err.println(e);
	    Token t;
	    do {
	        t = getNextToken();
	    }
	    while (t.kind != kind);
	}
	\end{lstlisting}
\end{frame}

\begin{frame}[t,fragile]{Deep Error Recovery}
	\begin{columns}
		\begin{column}{.3\linewidth}
			When error occurs (even deeper in the parse tree), we want to recover
		\end{column}
		\begin{column}{.7\linewidth}
			\vspace{-.5cm}
			\begin{lstlisting}[style=lststyle-java]
TOKEN : { <SEMICOLON: ";"> }
private void stm() :
{}
{ ifStm()
  | whileStm()
}
			\end{lstlisting}
			\mbox{}\hfill\pgfuseimage{bottom-arrow}\hfill\mbox{}
			\begin{lstlisting}[style=lststyle-java]
TOKEN : { <SEMICOLON: ";"> }
private void stm() :
{}
{ try {
      ifStm()
      | whileStm()
  } catch(ParseException e) {
      error_skipto(e, SEMICOLON);
  }
}
			\end{lstlisting}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[fragile]{Definition of the function \ccode{error\_skipto()}}
	\begin{lstlisting}[style=lststyle-java]
	//JAVACODE
	void error_skipto(ParseException e, int kind) {
	    System.out.println(e);
	    Token t;
	    do {
	        t = getNextToken();
	    }
	    while (t.kind != kind);
	}
	\end{lstlisting}
\end{frame}

\section{Conclusion}
\sectiontableofcontentslide

\begin{frame}{{Key Concepts} in the Chapter}
	\begin{small}
	\begin{description}
	\item[Parsers] A parser takes as input tokens from the lexical analyzer and treats the token names as terminal symbols of a context-free grammar. The parser then constructs a parse tree for its input sequence of tokens; the parse tree may be constructed figuratively or literally
	\item[Context-Free Grammars] A grammar specifies a set of terminal symbols (inputs), another set of nonterminals (symbols representing syntactic constructs), and a set of productions, each of which gives a way in which strings represented by one nonterminal can be constructed from terminal symbols and strings represented by certain other nonterminals. A production consists of a head (the nonterminal to be replaced) and a body (the replacing string of grammar symbols)
	\item[Derivations] The process of starting with the start-nonterminal of a grammar and successively replacing it by the body of one its productions is called derivation. If the leftmost (or rightmost) nonterminal is always replaced, then the derivation is called leftmost (resp. rightmost)
	\end{description}
\end{small}
\end{frame}

\begin{frame}{{Key Concepts} in the Chapter \insertcontinuationtext}
	\begin{small}
	\begin{description}
	\item[Parse Trees] A parse tree is a picture of a derivation, in which there is a node for each nonterminal that appears in the derivation. The children of a node are the symbols by which that nonterminal is replaced in the derivation. There is a one-to-one correspondence between parse trees, leftmost derivation, and rightmost derivations of the same terminal string
	\item[Ambiguity] A grammar for which some terminal string has two or more different parse tree is said to be ambiguous
	\item[Top-Down and Bottom-Up Parsing] Parsers are generally distinguished by whether they work top-down or bottom-up. Top-down parsers include recursive-descent and LL parsers, while the most common forms of bottom-up parsers are LR parsers
	\item[Design of Grammars] Grammars suitable for top-down parsing often are harder to design than those used by bottom-up parsers. It is necessary to eliminate left-recursion. We also must left-factor/group productions for the same nonterminal that have a common prefix in the body
	\end{description}
\end{small}
\end{frame}

\begin{frame}{{Key Concepts} in the Chapter \insertcontinuationtext}
\begin{small}
\begin{description}
	\item[Recursive-Descent Parsers] These parsers use a procedure for each nonterminal
	\item[$LL(1)$ Parsers] A grammar such that it is possible to choose the correct production with which to expand a given nonterminal, looking only at the next input symbol, is called $LL(1)$. These grammars allow us to construct a predictive parsing table that gives, for each nonterminal and each lookahead symbol, the correct choice of production
	\item[Shift-Reduce Parsing] Bottom-up parsers generally operate by choosing on the basis of the next input symbol and the contents of the stack, whether to shift the next input onto the stack, or to reduce some symbols at the top of the stack. A reduce takes a production body at the top of the stack and replaces it by the head of the production
	\item[Viable Prefixes] In shift-reduce parsing, the stack contents are always a viable prefix, ie. a prefix of some right-sentential form that ends no further right than the end of the handle of that right-sentential form. The handler is the substring that was introduced in the last step of the rightmost derivation of that sentential form
	\end{description}
\end{small}
\end{frame}

\begin{frame}{{Key Concepts} in the Chapter \insertcontinuationtext}
\begin{small}
\begin{description}
	\item[Valid Items] An item is a production with a dot somewhere in the body. An item is valid for a viable prefix if the production of that item is used to generate the handler, and the viable prefix includes all those symbols to the left of the dot
	\item[$LR$ Parsers] Each of the several kinds of LR parsers operate by first constructing the sets of valid items (called LR states) for all possible viable prefixes, and keeping track of the state for each prefix on the stack. The set of valid items guide the shift-reduce parsing decision
	\item[Simple $LR$ Parsers] In an $SLR$ parser, we perform a reduction implied by a valid item with a dot at the right end, provided the lookahead symbol can follow the head of that production in some sentential form
	\item[Canonical-$LR$ Parsers] This more complex form of $LR$ parser uses items that are augmented by the set of lookahead symbols that can follow the use of the underlying production. A canonical-$LR$ parser can avoid some of the parsing-action conflicts that are present in $SLR$ parsers, but often has many more states than the $SLR$ parser for the same grammar
	\end{description}
	\end{small}
\end{frame}

\begin{frame}[t,fancyframetitle=false,allowframebreaks]{\bibname\ of the Chapter}%
	\tiny%
	\putbib[chapters/chapter3/biblio]%
\end{frame}%

\end{bibunit}
\end{graphicspathcontext}
	
\endinput
